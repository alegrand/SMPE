#+TITLE:     Scientific Methodology and Performance Evaluation for Computer Scientists
#+AUTHOR:    Cyril Labbé, Arnaud Legrand, and Jean-Marc Vincent
#+DATE: October 2019
#+STARTUP: overview indent

#+BEGIN_QUOTE
*Reporting errors*: Although I do my best there may definitely be typos
and broken links. This is github so please report me everything you
find so that I can improve for others. :)
#+END_QUOTE

This webpage gathers information about the lectures given at the
University of Porto Alegre to the Master/PhD students in Computer
Science from October 7th 2019 to October 11th 2020.

* Useful links 
- Here is the [[http://pads.univ-grenoble-alpes.fr/p/MOSIG-SMPE-1920][Pad]] you will use to collaborate.
- Here is the [[https://edt.grenoble-inp.fr/2019-2020/exterieur/][lecture schedule on ADE]] (look for "scientific
  method"). A table summary is [[https://edt.grenoble-inp.fr/2019-2020/exterieur/jsp/custom/modules/plannings/eventInfo.jsp?week=-1&day=-1&slot=0&eventId=35927&activityId=-1&resourceId=-1&sessionId=-1&repetition=-1&order=slot&availableZone=-1][here]].
Links to the slides are provided below.

* Course Objective and Organization
The aim of this course is to provide the fundamental basis for sound
scientific methodology of performance evaluation of computer
systems. This lecture emphasize on methodological aspects of
measurement and on the statistics needed to analyze computer systems.
I first sensibilize the audience to the experiment and analysis
reproducibility issue in particular in computer science. Then I
present tools that help answering the analysis problem and may also
reveal useful for managing the experimental process through
notebooks. The audience is given the basis of probabilities and
statistics required to develop sound experiment designs. Unlike some
other lectures, my goal is not to provide analysis recipes that people
can readily apply but to make people really understand some simple
tools so that they can then dig deeper later on.

The lecture will be based on the following set of slides, which we will
probably adapt to the audience on the fly.
1. [[file:../../lectures/lecture_epistemology.pdf][Epistomology]]
2. [[file:../../lectures/lecture_reproducible_research.pdf][Reproducible research]]
3. [[file:../../lectures/lecture_literate_programming.pdf][Literate programming]] (Probably not as it will be explained in the
   "Reproducible research" slides)
4. [[file:../../lectures/lecture_R_crash_course.pdf][R crash course]] (Probably a live demo)
   - You may want to look at the [[*Learning R][Learning R]] section of this document
     or (probably better) follow this short series of tutorials from
     software carpentry which are quite pedagogical:
     - [[http://swcarpentry.github.io/r-novice-gapminder/01-rstudio-intro/][Introduction to R and Rstudio]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/09-vectorization/][Vectorization]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/06-data-subsetting/][Subsetting data]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/04-data-structures-part1/][R data structures 1]], [[http://swcarpentry.github.io/r-novice-gapminder/05-data-structures-part2/][Data structures 2 (data frames)]], although
       I will not go into as much detail as this tutorial in my crash
       course.
     - [[http://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2/][Beautiful graphics with ggplot2]].
   - For additional information, have a look at the "Using R" section
     below.
5. [[file:../../lectures/lecture_descriptive_univariate.pdf][Descriptive statistics of univariate data]] (Probably not as it will
   be evoked in the "Introduction to probabilities/statistics" slides
6. [[file:../../lectures/lecture_data_presentation.pdf][Data presentation]] (if time allows since it will be mentioned in the
   R crash course and in the "introduction to probabilities/statistics")
7. [[file:../../lectures/lecture_correlation_causation.pdf][Correlation and causation]] (Probably not as it will be evoked in the
   "Introduction to probabilities/statistics" slides)
8. [[file:../../lectures/3_introduction_to_statistics.pdf][Introduction to probabilities/statistics]] (if needed, [[file:../../lectures/lecture_central_limit_theorem.pdf][Proof of the
   Central limit theorem]] and [[file:../../lectures/lecture_chi_square.pdf][Unveiling the mysterious chi square
   test]]).
9. [[file:../../lectures/4_linear_model.pdf][Linear regression]]
10. [[file:../../lectures/5_design_of_experiments.pdf][Design of Experiments]]

All the examples given in this series of lecture use the [[http://www.r-project.org/][R]] language
and the source is provided so that people can reuse them. The slides
are composed with [[http://orgmode.org][org-mode]], beamer, and verbments.

* Course Tentative Schedule
1. 2019/10/03 @ 15:30: [AL] RR / Litterate Programming + MOOC à suivre
2. 2019/10/17 @ 15:30: Canceled. Spread on November lectures
3. 2019/10/24 @ 15:30: [AL] Introduction to R and the tidyverse 
4. 2019/11/07 @ 15:30: [JM] Visualizing data (with ggplot)
5. 2019/11/14 @ 15:30: [JM] Statistics: CLT + confidence interval
6. 2019/11/21 @ 15:30: [AL] Statistics: hypothesis, decision and test,
   "comparing" distribution
7. 2019/11/28 @ 15:30: [AL] Analysis Of Variance, Causality, Linear regression
8. 2019/12/05 @ 15:30: [AL] Linear regression (continuous)
9. 2019/12/12 @ 15:30: [AL] Design of Experiments
10. 2019/12/19 @ 15:30: [CL + JM] Epistemology 
11. 2020/01/09 @ 15:30: [CL] Publishing and Ethics
12. 2020/01/16 @ 15:30: [CL + AL + JM] Feedback on homework

* Hands-on
In the 3rd module of the [[https://www.fun-mooc.fr/courses/course-v1:inria+41016+session01bis/about][MOOC on Reproducible Research]], there is a
peer-reviewed homework that should allow you to practically use
everything you learnt. There are four proposed topics:
- (A) Concentration de CO2 dans l'atmosphère depuis 1958
- (B) Le pouvoir d'achat des ouvriers anglais du 16ème au 19ème siècle
- (C) L'épidémie de choléra à Londres en 1854
- (D) Estimation de la latence et de la capacité d’une connexion à partir de mesures asymétriques

You may propose additional topics to work on.

* Feedback on your hand-on
This is a subjective evaluation not a full-fledged feedback. I mostly
took notes so that we could discuss about all this during the lecture.
** Quentin Guilloteau: [[https://github.com/GuilloteauQ/SMPE][Carbon]] (A)
- Overall, it is a very nice work with sound models which are
  iteratively improved.
- I could re-execute your code and generate the report
- I would recommend that you conserve a copy of the data and of the
  generated output in your git repos (especially as Rmd does not
  include the output).
- If there is no =NA=, why do you still remove them with a =drop_na= ?
- Regarding =NA=, it turns out that some records are missing in this
  data set but that they are not indicated.
- Computing your prediction and then plotting it is very good (it
  allows you to inspect the output of the linear regression). You may
  have want to use =geom_smooth= though to display the uncertainty (on
  the expectation). Surprisingly, I did not manage to get =geom_smooth=
  to work with the sinusoidal fit (the frequency is wrong).
    #+begin_src R :results output :session *R* :exports both
    reg_quadsin <- lm(data = df, CO2 ~ poly(day, 2)+I(sin(day*2*pi/365.25))+I(cos(day*2*pi/365.25)))
    summary(reg_quadsin)
    #+end_src

    #+begin_src R :results output :session *R* :exports both
    data_prediction <- data.frame(day = seq(1, day_2025, 7))
    data_prediction$date <- as.Date(date_first_measure + data_prediction$day)
    data_prediction$CO2 <- predict(reg_quadsin, data_prediction)
  
    ggplot(data = df, aes(x = day, y = CO2)) +
       theme_linedraw() +
       geom_point(size = 0.1) +
       geom_smooth(aes(x = day), method="lm", formula = y~poly(x,2)+I(sin(x*2*pi/365.25))+I(cos(x*2*pi/365.25)), color = "red", fullrange=TRUE) + 
       geom_line(data = data_prediction, aes(x = day, y = CO2), color = "blue") + 
       xlab("Date") + ylab("CO2 (in ppm)") +
       ggtitle("CO2 Variations to the Present Day and its Estimation until 2025")
    #+end_src
- You should have looked at the global prediction, not just for a few
  years and also beyond 2020. This would have allowed you to see the
  impact of overfitting, even though the higher order terms appear significant...
- You may want to have a look at
  https://github.com/duvenaud/phd-thesis/blob/master/grammar.pdf,
  section 1.6.1 for an other kind of regression with an other way of
  computing uncertainty. (A+)
** Tijana Ninkovic: [[https://github.com/t-ninkovic/SMPE][Purchasing power of English workers from the 16th to the 19th century]] (A-)
- Nice work with explanations.
- Did not try to rerun but all this looks OK.
- In the second graph (with two different y-axis on the same graph),
  none of the scale start at 0. Don't you thing this has an
  impact. The real question, is probably "how can we compare such
  numbers. Dividing one by the other as you propose looks sound.
- Don't you think the y axis in the third graph should also start at 0
  ?
- Do you think wheat is a sufficient indicator of wealth ?
** Mihaela-Chavdarova Popova: [[https://github.com/MCPopova/SMPE][Purchasing power of English workers from the 16th to the 19th century]] (A)
- https://htmlpreview.github.io/?https://github.com/MCPopova/SMPE/blob/master/homework.nb.html
- Nice work, nice critical analysis of your visualisations.
- I also like the "time series" with the ratio better. Is it sound to
  add the =stat_smooth= though as a readers may want to interpret the
  gray area ?
** Loic Balleydier: [[https://app-learninglab.inria.fr/gitlab/d8fce4f34e466f9fa6d3dd39c6666de1/mooc-rr/blob/master/module3/exo3/exercice.ipynb][L'épidémie de choléra à Londres en 1854]] (Jupyter, Python 3) (C)
- *Adding the pip commands is good.*
- Missing data is not solely about "isnull"
- There is no visible map at all...
** Yang Tao Wang: [[https://github.com/CamosiWANG/SMPE/blob/master/TP%2520YangtaoWANG.pdf][The London cholera epidemic of 1854]] (A/A-)
- Nice work. You decided to hide the code and to leave only the data
  but this is ok as all the code underneath is available.
- I tried to reexecute and rerun your code but unfortunately, I got
  "Error in loadNamespace(name) : aucun package nommé ‘rgdal’ n'est
  trouvé".
- I was about to write it would be nice to explain how you got the
  data but you have put explanations in the =data/= repos
- You write you could "prove" the pump is in the center of the
  epidimy. I would rather use the term "show" or "illustrate". Anyway
  is it a causal relation ? There may be a bakery right in front of
  this pump which is at the origin of the epidemy. Even though you use
  modern computing and statistical tools, try to step back and ask
  yoursel what can be concluded ? Is the data reliable ? How could we
  determine if this pump isindeed the origin of the epidemy ?
** Juan-Fernando Vazquez Rodriguez: [[https://app-learninglab.inria.fr/gitlab/f5f24a51c38213cf87501637fb665929/mooc-rr/blob/master/module3/exo3/exercice.ipynb][Latency and capacity estimation for a network connection from asymmetric measurements]] (A/A+)
- I did not try to rerun but everything looks sound. *It's good you
  included the pip commands to install the packages you need.*
- *Identifying the break at 1480 is well done and justified.*
- Investigating the connection between the different regimes at
  different time intervals and message size was good. Although the
  system is not stationnary at "short" time scale, there does not seem
  to be much pattern between the two states so at a global scale it is
  reasonable.
- *You checked the R^2 and and the significance of your
  estimates. That's good.*
- *Being different from 0 is not the only important aspect.* It appears
  like the 99% CI for 1/C is [1.07447526e-04 5.45184375e-04], i.e. the
  bandwidth is between 1834 and 9345 bytes per milisecond. Doesn't
  this look like a rather poor estimate to you ?  Note that the
  estimate with the size2 dataset is even worse since infinite
  bandwidth would then appear as plausible. :) I know that's not what
  you said or meant. I'm just pointing it out.
- Comparing with the situation with the min, the 99% CI for 1/C is [
  [2.33039633e-04 2.41130739e-04]], i.e. the bandwidth is between and
  4147 and 4291 bytes per milisecond, which is a rather tight
  estimate. The estimate for larger size is betwen 3773 and 4752
  bytes/ms. The fact that the size range is smaller can explain this
  larger uncertainty.
- Note that you could try to do the fit on both data sets by imposing
  the offset to depend on the small/large feature.
- It would have be nice if you had try to explain why you seem to fail
  to correctly model the phenomenon for the remote server.
** Buchra Aboubakr: [[https://app-learninglab.inria.fr/gitlab/d612460c673a9c9113dd9d886f401f80/mooc-rr.git][Latency and capacity estimation for a network connection from asymmetric measurements]] (C)
- that's a start but there is not much for the moment
** Nicolas Amat: [[https://github.com/nicolasAmat/SMPE_Ping][Latency and capacity estimation for a network connection from asymmetric measurements]] (A/A+)
- Good work.
- I did not try to rerun but everything looks sound.
- Not your fault but I had troubles displaying/reading your notebook
  as it was too long.
- *Cheking for data integrity is a very good thing*
- Good handling of missing data
- When you write "No relation between the transmission time and the
  measurement date." This is not true. There is a relation but moving
  from a regime to an other appears random.
- *Outliers*: "For the first class we remove all the measurements with a
  transmission time larger than $30$ms." Same thing for the second
  class. Good, it is documented. Is this reasonable ? Does this have
  an impact ?
- Looking at the fit *without looking at the confidence interval* and
  the variability is dangerous/not very sound. Don't you find it
  strange that your bandwidth estimate for class 1 and 2 are so
  different from each others. Ooh, ok, you did it earlier.
- *Linear regression using boxplot for removing outliers*. OK, why not ?
  This is pragmatic but what are the underlying assumptions and why is
  it sound ?
- In the end, *it is nice as you investigated LM, IQR+LM, min+LM, and
  QR*. It would have been nice to summarize the CI on C for the four
  methods.
- "The first point on this study is that the first dataset seems to be
  more appropriate for the study of latency." I disagree with this
  statement. You don't get to choose the dataset. You get to choose
  the method.
- "We can also observe that the capacity C is much larger for the
  second connection, and so the capacity has a weaker impact on the
  transmission time." I also disagree with this statement. The latency
  is so large and the phenomenon so noisy (in particular as the
  duration is rounded to an integer number of miliseconds) that you
  cannot reliably estimate the bandwidth with any of the method you
  tried.
** Guy Kanbar: [[https://github.com/guykanbar/smpe][Latency and capacity estimation for a network connection from asymmetric measurements]] (A-)
- The *figure on page 4 is quite a good illustration of what happens*.
- Nice report, clean, well explained but you did not look at the
  output of the regression nor tried to provied an estimate of the
  bandwidth. If you had tried, you would have realized your estimate
  is not very reliable.
** Ana Khorguani: [[https://app-learninglab.inria.fr/gitlab/65ef26c6a1fd5adbb2734089645025c4/mooc-rr/blob/master/Project/Network.ipynb][Latency and capacity estimation for a network connection from asymmetric measurements]] (A/A-)
- Nice you used both python and R in the same notebook and it appeared to work quite well.
- "we see that Adjusted R-squared is almost 0, that tells us that the fit is very bad." Not exactly since the graph 19 shows a line which is not that bad.
- You did not only looked at the summary of the regression but also at the *four checking plots*, which is good. The ones for the regression of the min appear quite satisfying...
- "Equation based on this result is T(S) = 1.034 + 0.0002368 * S. That means that, with 95% confidence, latency is equal to 1.034 and bandwidth is 4222.9 bytes/s." Erm, no, you cannot state this this way. This formula is the maximum likelihood estimate under some (obviously wrong) assumptions and both estimates are unbiased estimators of the true values for which you can derive CI. But the 1/C is actually in [2.33E-4, 4.00E-4], i.e., the bandwidth is in [2500,4291] bytes/ms.
- *Plotting the different qr is a nice idea*. It would have been nice
  to look (using the confidence) on how the estimation of the capacity depends on the quantile.
** Salman Farhat: [[https://github.com/salmanfarhat1/SMPE-project][Latency and capacity estimation for a network connection from asymmetric measurements]] (A/A-)
- Good work. Explanations are clear although there are a few mistakes.
- Why would you supress the warning messages ? They may be important.
- *Why would you split the data at 1500* when your clearly see that the
  threshold is a bit smaller than this? This is likely to mess the
  regression and actually, after reading further, it does.
- Looking at the distribution of message size was a good idea. It indicates you that the experiment design is actually quite good...
- Your interpretation of the regression output is a bit "naive", e.g., "using Pr value, we see that size is an excellent addition to the model" and "the very low R^2 means that another factor should be taken into consideration."
- *Using an ANOVA to compare the two quantile regression* is a very good
  idea. I'm not sure about the underlying hypothesis though and I
  don't know if you know these hypothesis... Note that it is not
  surprising that the .75 quantile is different. Smaller thresholds may be of interest though.
- "We can see an interesting result in the Figure". What is it you
  have in mind ?
- There is something wrong with the regression of the minmum for class
  1, probably because of the wrong classification (1500) and you
  cannot see it because of your zoom with the =coord_cartesian=. The
  problem is also "visible" in the output of your regression where
  your R^2 is too low (.65 instead of .95).
** Marie Badaroux: [[https://github.com/MarieBadaroux/Recherche_Reproductible][Latency and capacity estimation for a network connection from asymmetric measurements]] (A-)
- Good work regarding RR although some statistical aspects are not well understood yet.
- Hehe, I see you now master regular expressions. https://xkcd.com/208/ ;)
- When you write "Do the variations of time only depend on the size of the packet? To answer, we need to continue our analysis.", you do not really answer this question. You should have looked whether message size was randomized or not e.g., by plotting them.
- Good identification of the 1480 threshold.
- "We can see that the capacity was divided by 10 and the latency is
  higher for the second part of the data. So when the packets are
  bigger, the performances decrease." No *you can't conclude this as
  the uncertainty on the estimate is very large* as indicated by the
  statsmodels output. This is why *sklearn is quite useless here*.
- "$R^2$ is equal to zero for the two parts of the data, so the linear
  regression is irrelevant." I disagree with this statement. It is
  more subtle than this.
- QR is "less sensitive to outliers" but it is foremost estimating a different aspect.
- argh "y_linear_reg.append(0.0002 * x + 1.0334)". The approximation on the intercept completely messes your graph. Why didn't you use predict ?
- If I may *python does not appear as very helpful for this work
  compared to R* (vander, sklearn vs. statsmodels, rounding, etc.)
** Vincent Jicquel: [[https://github.com/Jicquel/reproducible_research][Latency and capacity estimation for a network connection from asymmetric measurements]] (A+)
https://htmlpreview.github.io/?https://github.com/Jicquel/reproducible_research/blob/master/main.nb.html

- Apparently, there was missing data so something was wrong in your handling of this.
- Damn, *plotly* does not with with htmlpreview but it is indeed
  convenient to explore data
- "But in this case, there is not any easily predictible pattern."
  Indeed, this is the right formulation.
- Looking at the *corelation* is a good idea but is far from
  sufficient.
- Good handling of the threshold detection.
- Trying to *handle overplotting* with =geom_count= and transparancy is a
  good idea.
- Good use of the colour class/aes to have both regressions on the
  same graph.
- "For the second class, the linear regression is totally
  inappropriate, the slope, the standard error and R-squared are bad."
  I wouldn't call it inappropriate as the phenomenon is still linear.
- It would have been nice to plot the min+Lm regression.
- *Nice plot of the ci on rq*.
- I'm not sure it makes much sense though to focus on tau=0.8.
- Be careful with *your capacity estimates that do not take the CI
  into account*.
  - Note that if you have no good reason to assume that the latency is
    different, you could fit with a latency term that depends on the
    class.
- I fully agree with your conclusion.
** Abdelwahab Sadki: [[https://github.com/SADKIAbdel/Linear-Regression-Project][Latency and capacity estimation for a network connection from asymmetric measurements]] (A-)
- I don't really understand what you mean when you write "NB: in the
  markdown file, I did the linear regression it was too bad. but i
  wanted to have something to compare the next models with maybe it
  will give something useful for understanding the problematic
  better"...
- Good work although there are a few strange statements that make me
  feel like some statistical aspects are not fully understood yet.
- *Good, you looked at the residuals with the 4 plots*.
  - I would not call it "uniformly distributed data though"
  - Slope 3.243e-04 \pm 8.497e-05. Do not forget to double it for a 95%
    CI. This is strange as you did it for the intercept...
  - Capacity is not 1/intercept
- Other weird statement: "we have a good probability of the intercept
  and the slope to be absurde. also the p-value is small so we can
  trust this values".
- I understand you are annoyed by the minimum selection but if your
  first argument is very sound (we discard many values), your second
  argument is not really clear.
- See how the *0 and 1 quantiles are "weird"*.
- "the quantile regression can show us what's wrong with our
  experiments and the measurements that we need to ignore". Like what
  ? This is not clear.
- "..., which means that there is another parameter to take in
  consideration to build the model". Are you sure this is the where
  problem comes from ? I don't think so.
** Maiko Muller: [[http://htmlpreview.github.io/?https://app-learninglab.inria.fr/gitlab/c26d5172d5d24d037fe4cd5e4931b9f6/moocrr-reproducibility-study/raw/master/src/Rust/Project.html][Reproducing a paper on the Challenger Dataset with Rust]] (A+)
- Using rust for this is crazy. :) I'm impressed you managed and
  reporting how you did proceed is very interesting. Good work.
- We already talked about it. You had to reimplement most of the code
  and you managed to compute the CI of the coefficients. Computing the
  CI for each temperature requires some (similar) extra-work but you
  were right to stop there. You've been quite pugnacious. :) That's
  quite impressive overall.
** Alaa Zbair: [[https://github.com/alaazbair/SMPE][Predict portion of time that cpus run in user mode from system performance measures]] (A--)
- A Sun Sparcstation 20/712 ?!? Wow, I feel like I jumped back in time
  more than 20 years ago. :) Where did this come from ? A Kaggle
  dataset ?
- The whole report is very well written and _looks_ rigorous but there
  are sometimes very weird statements and I had a lot of trouble
  following your thoughts and your methodology. This seems overall
  quite confused and I don't think I would trust your conclusions.
- "if usr, runsqz follow a normal distribution we can conclude there
  is a dependance relationship between the two features ?" Where
  does this come from ?
- When performing a *exploratory data analysis*, I would suggest using
  the classical multi-dimensional plot of R, which is also available
  through ggplot with ggpairs. This would have allowed you to look at
  all distributions at once instead of looking only at usr, runsqz and
  freeswap.
- Although your plots are very nice and composed with a lot of care
  (e.g. the one on page 6), I do not think they bring that much useful
  information: you are overlaying a histogram, a histogram and a
  boxplot but we don't know what's of interest.
- You mention in the introduction the dataset is composed of two
  separate occasions but do not investigate it. Why does it make sense
  to model usr as a function of the other variables ?
- I do not see why rescaling the data is important if you intend to
  fit linear model.
- Why would you print =summary(model)$coefficients= instead of
  =summary(model)=. The latter one is visually much more readable/useful
  in such context.
- Note that an anova is not the same as a linear morel and the
  interpretation of the test is different. I fear that what you write
  is not really meaningful.
** Henrik Helenius
** Manal Benaissa
** Thibault Lacharme
** soukayna omrachi
** Gabriel Benevides
** Johana Marku
** Jonathan Borne

* Requirements 
All the examples given in this series of lecture use the [[http://www.r-project.org/][R]] language
and the source is provided so that people can reuse them. The slides
are composed with [[http://orgmode.org][org-mode]], beamer, and verbments.

It is not expected that students already knows the R language as I
will briefly present it. However, they should have already installed
Rstudio and R (check the next section if you need information) on
their laptop so as to try out the examples I provide for themselves.

Alternatively, you may use python with Jupyter. Most R verbs have now
their python counterpart (e.g., =numpy= and =pandas= for vectors and
dataframes, =plotnine= for =ggplot2=, =statsmodels= for linear regressions,
...).
* Using R
** Installing R and Rstudio
Here is how to proceed on debian-based distributions:
#+BEGIN_SRC sh
sudo apt-get install r-base r-cran-ggplot2 r-cran-reshape r-cran-knitr r-cran-magrittr
#+END_SRC
Make sure you have a recent (>= 3.2.0) version or R. For example, here
is what I have on my machine:
#+begin_src sh :results output :exports both
R --version
#+end_src

#+RESULTS:
#+begin_example
R version 3.5.1 (2018-07-02) -- "Feather Spray"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.

#+end_example

Rstudio and knitr are unfortunately not packaged within debian so the
easiest is to download the corresponding debian package on the [[http://www.rstudio.com/ide/download/desktop][Rstudio
webpage]] and then to install it manually (depending on when you do this
and on the version of your OS, *you should obviously change the version
number*).

#+BEGIN_SRC sh
wget https://download1.rstudio.org/rstudio-xenial-1.1.456-amd64.deb
sudo dpkg -i rstudio-xenial-1.1.456-amd64.deb
sudo apt-get -f install # to fix possibly missing dependencies
#+END_SRC
You will also need to install knitr. To this end, you should simply
run R (or Rstudio) and use the following command.
#+BEGIN_SRC R
install.packages("knitr")
#+END_SRC
If =r-cran-ggplot2= or =r-cran-reshape= could not be installed for some
reason, you can also install it through R by doing:
#+BEGIN_SRC R
install.packages("ggplot2")
install.packages("reshape")
#+END_SRC
** Producing documents
The easiest way to go is probably to [[http://www.rstudio.com/ide/docs/authoring/using_markdown][use R+Markdown (Rmd files) in
Rstudio]] and to export them via [[http://www.rpubs.com/][Rpubs]] to make available [[http://www.rpubs.com/tucano/zombies][whatever you
want]].

We can roughly distinguish between three kinds of documents:
1. Lab notebook (with everything you try and that is meant mainly
   for yourself)
2. Experimental report (selected results and explanations with
   enough details to discuss with your advisor)
3. Result description (rather short with only the main point and,
   which could be embedded in an article)
We expect you to provide us the last two ones and to make them
publicly available so as to allow others to [[http://rpubs.com/RobinLovelace/ratmog11][comment]] on them.
** Learning R
For a quick start, you may want to look at [[http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf][R for Beginners]]. A probably
more entertaining way to go is to follow a good online lecture
providing an introduction to R and to data analysis such as this one:
https://www.coursera.org/course/compdata. 

A quite effective way is to use [[http://swirlstats.com/students.html][SWIRL]], an interactive learning
environment that will guide through self-paced lesson.
#+begin_src R :results output :session :exports both
install.packages("swirl")
library(swirl)
install_from_swirl("R Programming")
swirl()
#+end_src
I suggest in particular to follow the following lessons from R
programming (max 10 minutes each):
#+BEGIN_EXAMPLE
 1: Basic Building Blocks      2: Workspace and Files     
 3: Sequences of Numbers       4: Vectors                 
 5: Missing Values             6: Subsetting Vectors      
 7: Matrices and Data Frames   8: Logic                   
 9: Functions                 12: Looking at Data         
#+END_EXAMPLE

Finally, you may want to read this [[http://ww2.coastal.edu/kingw/statistics/R-tutorials/dataframes.html][excellent tutorial on data frames]]
(=attach=, =with=, =rownames=, =dimnames=, notions of scope...).
** Learning ggplot2, plyr/dplyr, reshape/tidyR
All these packages have been developed by hadley wickam.
- Although the package is called =ggplot2=, it provides you the =ggplot=
  command. This package allows you to produce nice looking and highly
  configurable graphics.
- Old generation: =plyr= allows you expressively compute aggregate
  statistics on your data-frames and =reshape= allows you to reshape
  your data-frames if they're not in the right shape for =ggplot2= or
  =plyr=. Hence, don't use it unless you are definitely stuck with a
  very old version of R.
- New generation: =dplyr= is the new generation of =plyr= and allows you
  to expressively compute aggregate statistics on your
  data-frames. =tidyr= is the new generation of =reshape= and allows you
  to reshape your data-frames if they're not in the right shape for
  =ggplot2= or =dplyr=. If you have a recent R installation, go for these
  new packages. Their syntax is better and their implementation is
  much faster.

I recently stumbled on this [[http://seananderson.ca/ggplot2-FISH554/][nice ggplot2 tutorial]].

Hadley Wickam provides a [[https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html][nice tour of dplyr]] and [[http://blog.rstudio.org/2014/07/22/introducing-tidyr/][gentle introduction to
tidyR]]. Here is a nice link on [[https://stat545-ubc.github.io/bit001_dplyr-cheatsheet.html][merging data frames]].

The Rstudio team has designed a [[https://www.rstudio.com/resources/cheatsheets/][nice series of cheatsheets on R]] and in
particular one on [[https://www.rstudio.com/wp-content/uploads/2015/05/ggplot2-cheatsheet.pdf][ggplot2]] and on [[https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf][R/markdown/knitr]].
* References
+ R. Jain, [[http://www.cs.wustl.edu/~jain/books/perfbook.htm][The Art of Computer Systems Performance Analysis:
  Techniques for Experimental Design, Measurement, Simulation, and
  Modeling]], Wiley-Interscience, New York, NY, April 1991.
  [[http://www.amazon.com/Art-Computer-Systems-Performance-Analysis/dp/1118858425/ref%3Dsr_1_2?s%3Dbooks&ie%3DUTF8&qid%3D1435137636&sr%3D1-2&keywords%3Dperformance%2Bmeasurement%2Bcomputer][A new edition will be available in September 2015]].
  #+BEGIN_QUOTE
  This is an easy-to-read self-content book for practical performance
  evaluation. The numerous checklists make it a great book for
  engineers and every CS experimental scientist should have read it.
  #+END_QUOTE
+ David J. Lilja, Measuring Computer Performance: A Practitioner’s
  Guide, Cambridge University Press 2005
  #+BEGIN_QUOTE
  A short book suited for brief presentations. I follow a similar
  organization but I really don't like the content of this book. I
  feel it provides very little insight on why the theory applies or
  not. I also think it is too general and lacks practical examples. It
  may be interesting for those willing a quick and broad presentation
  of the main concepts and "recipes" to apply.
  #+END_QUOTE
+ Jean-Yves Le Boudec. [[http://www.cl.cam.ac.uk/~dq209/others/perf.pdf][Methods, practice and theory for the
  performance evaluation of computer and communication
  systems, 2006. EPFL electronic book]].
  #+BEGIN_QUOTE
  A very good book, with a much more theoretical treatment than the
  Jain. It goes way farther on many aspects and I can only recommand
  it.
  #+END_QUOTE
+ Douglas C. Montgomery, [[http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002024.html][Design and Analysis of Experiments]], 8th
  Edition. Wiley 2013.
  #+BEGIN_QUOTE
  This is a good and thorough textbook on design of experiments. It's
  so unfortunate it relies on "exotic" softwares like JMP and minitab
  instead of R...
  #+END_QUOTE
+ Julian J. Faraway, [[https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf][Practical Regression and Anova using R]],
  University of Bath, 2002.
  #+BEGIN_QUOTE
  This book is derived from material that Pr. Faraway used in a Master
  level class on Statistics at the University of Michigan. It is
  mathematically involved but presents in details how linear
  regression, ANOVA work and can be done with R. It works out many
  examples in details and is very pleasant to read. A must-read if you
  want to understand this topic more thoroughly.
  #+END_QUOTE
+ Peter Kosso, [[http://www.amazon.fr/Summary-Scientific-Method-Peter-Kosso-ebook/dp/B008D5IYU2][A Summary of Scientific Method]], Springer, 2011. [[[http://hemija.pmf.ukim.edu.mk/materials/download/6d31fd3f53a82da9de163833806722ae][hidden
  PDF that google found on the webpage of a university in Macedonia]]
  #+BEGIN_QUOTE
  A short nice book summarizing the main steps of the scientific
  method and why having a clear definition is not that simple. It
  illustrates these points with several nice historical examples that
  allow the reader to take some perspective on this epistemological
  question.
  #+END_QUOTE
+ R. Nelson, Probability stochastic processes and queuing theory: the
  mathematics of computer performance modeling. Springer Verlag 1995.
  #+BEGIN_QUOTE
  For those willing to know more about queuing theory.
  #+END_QUOTE
