#+TITLE:     Scientific Methodology and Performance Evaluation for Computer Scientists
#+AUTHOR:    Arnaud Legrand
#+DATE: September 2018
#+STARTUP: overview indent

#+BEGIN_QUOTE
*Reporting errors*: Although I do my best there may definitely be typos
and broken links. This is github so please report me everything you
find so that I can improve for others. :)
#+END_QUOTE

This webpage gathers information about the lectures given at the
University of Rennes at the 2nd year Master students in Computer
Science in September 2018.

* Useful links 
- Here is the [[https://pad.inria.fr/p/np_UjaCOYWwyhQkUUK3][Pad]] you will use to collaborate.
- Here is the visio-conference link: https://rendez-vous.renater.fr/SMPE_Rennes_18
# - Here is a [[https://sondages.inria.fr/index.php/646461/lang-en][poll]]. Please fill it as soon as you have a few minutes but
#   do not waste your time doing it during the lecture.

Links to the slides are provided below.

* Course Objective and Organization
The aim of this course is to provide the fundamental basis for sound
scientific methodology of performance evaluation of computer
systems. This lecture emphasize on methodological aspects of
measurement and on the statistics needed to analyze computer systems.
I first sensibilize the audience to the experiment and analysis
reproducibility issue in particular in computer science. Then I
present tools that help answering the analysis problem and may also
reveal useful for managing the experimental process through
notebooks. The audience is given the basis of probabilities and
statistics required to develop sound experiment designs. Unlike some
other lectures, my goal is not to provide analysis recipes that people
can readily apply but to make people really understand some simple
tools so that they can then dig deeper later on.

The lecture will be based on the following set of slides, which I will
probably adapt to the audience on the fly.
1. [[file:../../lectures/lecture_epistemology.pdf][Epistomology]]
2. [[file:../../lectures/lecture_reproducible_research.pdf][Reproducible research]]
3. [[file:../../lectures/lecture_literate_programming.pdf][Literate programming]] (Probably not as it will be explained in the
   "Reproducible research" slides)
4. [[file:../../lectures/lecture_R_crash_course.pdf][R crash course]] (Probably a live demo)
   - You may want to look at the [[#learning-r][Learning R]] section of this document
     or (probably better) follow this short series of tutorials from
     software carpentry which are quite pedagogical:
     - [[http://swcarpentry.github.io/r-novice-gapminder/01-rstudio-intro/][Introduction to R and Rstudio]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/09-vectorization/][Vectorization]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/06-data-subsetting/][Subsetting data]]
     - [[http://swcarpentry.github.io/r-novice-gapminder/04-data-structures-part1/][R data structures 1]], [[http://swcarpentry.github.io/r-novice-gapminder/05-data-structures-part2/][Data structures 2 (data frames)]], although
       I will not go into as much detail as this tutorial in my crash
       course.
     - [[http://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2/][Beautiful graphics with ggplot2]].
5. [[file:../../lectures/lecture_descriptive_univariate.pdf][Descriptive statistics of univariate data]] (Probably not as it will
   be evoked in the "Introduction to probabilities/statistics" slides
6. [[file:../../lectures/lecture_data_presentation.pdf][Data presentation]] (if time allows since it will be mentioned in the
   R crash course and in the "introduction to probabilities/statistics")
7. [[file:../../lectures/lecture_correlation_causation.pdf][Correlation and causation]] (Probably not as it will be evoked in the
   "Introduction to probabilities/statistics" slides)
8. [[file:../../lectures/3_introduction_to_statistics.pdf][Introduction to probabilities/statistics]] (if needed, [[file:../../lectures/lecture_central_limit_theorem.pdf][Proof of the
   Central limit theorem]] and [[file:../../lectures/lecture_chi_square.pdf][Unveiling the mysterious chi square
   test]]).
9. [[file:../../lectures/4_linear_model.pdf][Linear regression]]
10. [[file:../../lectures/5_design_of_experiments.pdf][Design of Experiments]]

All the examples given in this series of lecture use the [[http://www.r-project.org/][R]] language
and the source is provided so that people can reuse them. The slides
are composed with [[http://orgmode.org][org-mode]], beamer, and verbments.

* Hands-on
Travaillez en binômes. Choisissez un des sujets dans la liste
ci-dessous et préparez une analyse des données associées sous forme
d'un document computationnel. Si le temps le permet, ce document sera
évalué par deux autres personnes.

Vous déposerez votre document omputationnel dans un dépot GitHub/GitLab/... sous forme de
notebook Jupyter (fichier .ipynb), de fichier Rmd + PDF si vous
utilisez RStudio, de fichier org avec les images générées pour les
plots si vous utilisez Emacs/Org-Mode.

Votre objectif principal est de convaincre les relecteurs potential de
votre document de la qualité de votre travail. Faites attention à bien
expliquer votre raisonnement et vos calculs !
** (A) Concentration de CO2 dans l'atmosphère depuis 1958
Prérequis : traitement de suites chronologiques

Une des premières observations systématiques du changement climatique
est la mesure de la concentration de CO2 dans l'atmosphère à
l'observatoire de Mauna Loa, Hawaii, États-Unis. Elle a été initiée
par Charles David Keeling en 1958, et continue encore à ce jour. En
honneur à Keeling, ce jeux de données est souvent appellé "Keeling
Curve" (voir https://en.wikipedia.org/wiki/Keeling_Curve pour
l'histoire et l'importance de ces données).

Les données sont disponibles sur le site Web de l'institut Scripps:
http://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record. Utilisez
le fichier avec les observations hebdomadaires. Attention, ce fichier
est mis à jour régulièrement avec de nouvelles observations. Notez
donc bien la date du téléchargement, et gardez une copie locale de la
version précise que vous analysez. Faites aussi attention aux données
manquantes.

Votre mission si vous l'acceptez:
1. Réalisez un graphique qui vous montrera une oscillation périodique
   superposée à une évolution systématique plus lente.
2. Séparez ces deux phénomènes. Caractérisez l'oscillation
   périodique. Fittez un modèle simple à la contribution lente et
   tentez une extrapolation jusqu'à 2025.
** (B) Le pouvoir d'achat des ouvriers anglais du 16ème au 19ème siècle
Prérequis : techniques de présentation graphique

[[https://fr.wikipedia.org/wiki/William_Playfair][William Playfair]] était un des pionniers de la présentation graphique
des données. Il est notamment considéré comme l'inventeur de
l'histogramme. Un de ses graphes célèbres, tiré de son livre "[[https://books.google.fr/books/about/A_Letter_on_Our_Agricultural_Distresses.html?id=aQZGAQAAMAAJ][A Letter
on Our Agricultural Distresses, Their Causes and Remedies]]", montre
[[https://fr.wikipedia.org/wiki/William_Playfair#/media/File:Chart_Showing_at_One_View_the_Price_of_the_Quarter_of_Wheat,_and_Wages_of_Labour_by_the_Week,_from_1565_to_1821.png][l'évolution du prix du blé et du salaire moyen entre 1565 et
1821]]. Playfair n'a pas publié les données numériques brutes qu'il a
utilisées, car à son époque la réplicabilité n'était pas encore
considérée comme essentielle. Des [[https://vincentarelbundock.github.io/Rdatasets/doc/HistData/Wheat.html][valeurs obtenues par numérisation du
graphe]] sont aujourd'hui téléchargeables, [[https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/HistData/Wheat.csv][la version en format CSV]]
étant la plus pratique.

Quelques remarques pour la compréhension des données:
- Jusqu'en 1971, la livre sterling était divisée en 20 shillings, et
  un shilling en 12 pences.
- Le prix du blé est donné en shillings pour un quart de boisseau de
  blé. Un quart de boisseau équivaut 15 livres britanniques ou 6,8 kg.
- Les salaires sont donnés en shillings par semaine.

Votre mission si vous l'acceptez:
1. Votre première tâche est de reproduire le graphe de Playfair à
   partir des données numériques. Représentez, comme Playfair, le prix
   du blé par des barres et les salaires par une surface bleue
   délimitée par une courbe rouge. Superposez les deux de la même
   façon dans un seul graphique. Le style de votre graphique pourra
   rester différent par rapport à l'original, mais l'impression
   globale devrait être la même.
2. Par la suite, améliorez la présentation des ces données. Pour
   commencer, Playfair a combiné les deux quantités dans un même
   graphique en simplifiant les unités "shillings par quart de
   boisseau de blé" et "shillings par semaine" à un simple
   "shillings", ce qui aujourd'hui n'est plus admissible. Utilisez
   deux ordonnées différentes, une à gauche et une à droite, et
   indiquez les unités correctes. À cette occasion, n'hésitez pas à
   proposer d'autres représentations que des barres et des
   surface/courbes pour les deux jeux de données si ceci vous paraît
   judicieux.
3. L'objectif de Playfair était de montrer que le pouvoir d'achat des
   ouvriers avait augmenté au cours du temps. Essayez de mieux faire
   ressortir cette information. Pour cela, faites une représentation
   graphique du pouvoir d'achat au cours du temps, définie comme la
   quantité de blé qu'un ouvrier peut acheter avec son salaire
   hebdomadaire. Dans un autre graphique, montrez les deux quantités
   (prix du blé, salaire) sur deux axes différents, sans l'axe du
   temps. Trouvez une autre façon d'indiquer la progression du temps
   dans ce graphique. Quelle représentation des données vous paraît la
   plus claire ? N'hésitez pas à en proposer d'autres.
** (C) L'épidémie de choléra à Londres en 1854
Prérequis : représentation de données géographiques

En 1854, le quartier de Soho à Londres a vécu [[https://fr.wikipedia.org/wiki/%25C3%2589pid%25C3%25A9mie_de_chol%25C3%25A9ra_de_Broad_Street][une des pires épidémies
de choléra du Royaume-Uni]], avec 616 morts. Cette épidémie est devenue
célèbre à cause de l'analyse détaillée de ses causes réalisée par le
médecin [[http://www.johnsnowsociety.org/][John Snow]]. Ce dernier a notamment montré que le choléra est
transmis par l'eau plutôt que par l'air, ce qui était la théorie
dominante de l'époque.

Un élément clé de cette analyse était une [[https://commons.wikimedia.org/wiki/File:Snow-cholera-map-1.jpg][carte]] sur laquelle John Snow
avait marqué les lieux des décès et les endroits où se trouvaient les
pompes à eau publiques. Ces données sont aujourd'hui [[http://blog.rtwilson.com/john-snows-cholera-data-in-more-formats/][disponibles sous
forme numérique]]. Nous vous proposons de les utiliser pour recréer la
carte de John Snow dans un document computationnel réplicable.

Votre mission si vous l'acceptez :
1. Londres a bien sûr évolué depuis 1854, mais une carte d'aujourd'hui
   est tout à fait utilisable comme support pour ces données
   historiques. À partir des données numériques, réalisez une carte
   dans l'esprit de celle de John Snow. Montrez les lieux de décès
   avec des symboles dont la taille indique le nombre de
   décès. Indiquez sur la même carte la localisation des pompes en
   utilisant une autre couleur et/ou un autre symbole. En R, nous vous
   suggérons l'utilisation de la bibliothèque [[https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf][ggmap]]. En Python, [[https://github.com/vgm64/gmplot][gmplot]]
   devrait faire l'affaire.
2. Par la suite, essayez de trouver d'autres façons pour montrer que
   la pompe de Broad Street est au centre de l'épidémie. Vous pouvez
   par exemple calculer la densité des décès dans le quartier et
   l'afficher sur la carte, mais n'hésitez pas à expérimenter avec
   d'autres approches.
** (D) Estimation de la latence et de la capacité d’une connexion à partir de mesures asymétriques
Prérequis : régression linéaire

Un modèle simple et fréquemment utilisé pour décrire la performance
d'une connexion de réseau consiste à supposer que le temps d'envoi /T/
pour un message dépend principalement de sa taille /S/ (nombre d'octets)
et de deux grandeurs propres à la connexion : la latence /L/ (en
secondes) et la capacité /C/ (en octets/seconde). La relation entre ces
quatre quantités est /T(S) = L + S/C/. Ce modèle néglige un grand nombre
de détails. D'une part, /L/ et /C/ dépendent bien sûr du protocole de
communication choisi mais aussi dans une certaine mesure de /S/. D'autre
part, la mesure de /T(S)/ comporte en général une forte composante
aléatoire. Nous nous intéressons ici au temps moyen qu'il faut pour
envoyer un message d'une taille donnée.

Votre tâche est d'estimer /L/ et /C/ à partir d'une série d'observations
de /T/ pour des valeurs différentes de /S/. Préparez votre analyse sous
forme d'un document computationnel réplicable qui commence avec la
lectures des données brutes, disponibles pour deux connexions
différentes, qui ont été obtenues avec l'outil =ping= :

- Le premier jeu de données examine une connexion courte à l'intérieur
  d'un campus:
  http://mescal.imag.fr/membres/arnaud.legrand/teaching/2014/RICM4_EP_ping/liglab2.log.gz
- Le deuxième jeu de données mesure la performance d'une connexion
  vers un site Web éloigné assez populaire et donc chargé:
  http://mescal.imag.fr/membres/arnaud.legrand/teaching/2014/RICM4_EP_ping/stackoverflow.log.gz

Les deux fichiers contiennent la sortie brute de l'outil =ping= qui a
été exécuté dans une boucle en variant de façon aléatoire la taille du
message. Chaque ligne a la forme suivante:
#+BEGIN_SRC
[1421761682.052172] 665 bytes from lig-publig.imag.fr (129.88.11.7): icmp_seq=1 ttl=60 time=22.5 ms
#+END_SRC

Au début, entre crochet, vous trouvez la date à laquelle la mesure a
été prise, exprimée en secondes depuis le 1er janvier 1970. La taille
du message en octet est donnée juste après, suivie par le nom de la
machine cible et son adresse IP, qui sont normalement identiques pour
toutes les lignes à l'intérieur d'un jeu de données. À la fin de la
ligne, nous trouvons le temps d'envoi (aller-retour) en
millisecondes. Les autres indications, =icmp_seq= et =ttl=, n'ont pas
d'importance pour notre analyse. Attention, il peut arriver qu'une
ligne soit incomplète et il faut donc vérifier chaque ligne avant d'en
extraire des informations !

Votre mission si vous l'acceptez :
1. Vous commencerez par travailler avec le premier jeu de données
   (liglab2). Représentez graphiquement l'évolution du temps de
   transmission au cours du temps (éventuellement à différents
   instants et différentes échelles de temps) pour évaluer la
   stabilité temporelle du phénomène. Ces variations peuvent-elles
   être expliquées seulement par la taille des messages ?
2. Représentez le temps de transmission en fonction de la taille des
   messages. Vous devriez observer une "rupture", une taille à partir
   de la quelle la nature de la variabilité change. Vous estimerez
   (graphiquement) cette taille afin de traiter les deux classes de
   tailles de message séparément.
3. Effectuez une régression linéaire pour chacune des deux classes et
   évaluez les valeurs de /L/ et de /C/ correspondantes. Vous superposerez
   le résultat de cette régression linéaire au graphe précédent.
4. (Optionnel) La variabilité est tellement forte et asymétrique que
   la régression du temps moyen peut être considérée comme peu
   pertinente. On peut vouloir s'intéresser à caractériser plutôt le
   plus petit temps de transmission. Une approche possible consiste
   donc à filtrer le plus petit temps de transmission pour chaque
   taille de message et à effectuer la régression sur ce sous-ensemble
   de données. Cela peut également être l'occasion pour ceux qui le
   souhaitent de se familiariser avec la [[https://fr.wikipedia.org/wiki/R%25C3%25A9gression_quantile][régression de quantiles]]
   (implémentée en R dans la bibliothèque quantreg et en Python dans
   la bibliothèque statsmodels).
5. Répétez les étapes précédentes avec le second jeu de données
   (stackoverflow).
* Requirements 
All the examples given in this series of lecture use the [[http://www.r-project.org/][R]] language
and the source is provided so that people can reuse them. The slides
are composed with [[http://orgmode.org][org-mode]], beamer, and verbments.

It is not expected that students already knows the R language as I
will briefly present it. However, they should have already installed
Rstudio and R (check the next section if you need information) on
their laptop so as to try out the examples I provide for themselves.
* Using R
** Installing R and Rstudio
Here is how to proceed on debian-based distributions:
#+BEGIN_SRC sh
sudo apt-get install r-base r-cran-ggplot2 r-cran-reshape r-cran-knitr r-cran-magrittr
#+END_SRC
Make sure you have a recent (>= 3.2.0) version or R. For example, here
is what I have on my machine:
#+begin_src sh :results output :exports both
R --version
#+end_src

#+RESULTS:
#+begin_example
R version 3.5.1 (2018-07-02) -- "Feather Spray"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.

#+end_example

Rstudio and knitr are unfortunately not packaged within debian so the
easiest is to download the corresponding debian package on the [[http://www.rstudio.com/ide/download/desktop][Rstudio
webpage]] and then to install it manually (depending on when you do this
and on the version of your OS, you can obviously change the version
number).

#+BEGIN_SRC sh
wget https://download1.rstudio.org/rstudio-xenial-1.1.456-amd64.deb
sudo dpkg -i rstudio-xenial-1.1.456-amd64.deb
sudo apt-get -f install # to fix possibly missing dependencies
#+END_SRC
You will also need to install knitr. To this end, you should simply
run R (or Rstudio) and use the following command.
#+BEGIN_SRC R
install.packages("knitr")
#+END_SRC
If =r-cran-ggplot2= or =r-cran-reshape= could not be installed for some
reason, you can also install it through R by doing:
#+BEGIN_SRC R
install.packages("ggplot2")
install.packages("reshape")
#+END_SRC
** Producing documents
The easiest way to go is probably to [[http://www.rstudio.com/ide/docs/authoring/using_markdown][use R+Markdown (Rmd files) in
Rstudio]] and to export them via [[http://www.rpubs.com/][Rpubs]] to make available [[http://www.rpubs.com/tucano/zombies][whatever you
want]].

We can roughly distinguish between three kinds of documents:
1. Lab notebook (with everything you try and that is meant mainly
   for yourself)
2. Experimental report (selected results and explanations with
   enough details to discuss with your advisor)
3. Result description (rather short with only the main point and,
   which could be embedded in an article)
We expect you to provide us the last two ones and to make them
publicly available so as to allow others to [[http://rpubs.com/RobinLovelace/ratmog11][comment]] on them.
** Learning R
For a quick start, you may want to look at [[http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf][R for Beginners]]. A probably
more entertaining way to go is to follow a good online lecture
providing an introduction to R and to data analysis such as this one:
https://www.coursera.org/course/compdata. 

A quite effective way is to use [[http://swirlstats.com/students.html][SWIRL]], an interactive learning
environment that will guide through self-paced lesson.
#+begin_src R :results output :session :exports both
install.packages("swirl")
library(swirl)
install_from_swirl("R Programming")
swirl()
#+end_src
I suggest in particular to follow the following lessons from R
programming (max 10 minutes each):
#+BEGIN_EXAMPLE
 1: Basic Building Blocks      2: Workspace and Files     
 3: Sequences of Numbers       4: Vectors                 
 5: Missing Values             6: Subsetting Vectors      
 7: Matrices and Data Frames   8: Logic                   
 9: Functions                 12: Looking at Data         
#+END_EXAMPLE

Finally, you may want to read this [[http://ww2.coastal.edu/kingw/statistics/R-tutorials/dataframes.html][excellent tutorial on data frames]]
(=attach=, =with=, =rownames=, =dimnames=, notions of scope...).
** Learning ggplot2, plyr/dplyr, reshape/tidyR
All these packages have been developed by hadley wickam.
- Although the package is called =ggplot2=, it provides you the =ggplot=
  command. This package allows you to produce nice looking and highly
  configurable graphics.
- Old generation: =plyr= allows you expressively compute aggregate
  statistics on your data-frames and =reshape= allows you to reshape
  your data-frames if they're not in the right shape for =ggplot2= or
  =plyr=. Hence, don't use it unless you are definitely stuck with a
  very old version of R.
- New generation: =dplyr= is the new generation of =plyr= and allows you
  to expressively compute aggregate statistics on your
  data-frames. =tidyr= is the new generation of =reshape= and allows you
  to reshape your data-frames if they're not in the right shape for
  =ggplot2= or =dplyr=. If you have a recent R installation, go for these
  new packages. Their syntax is better and their implementation is
  much faster.

I recently stumbled on this [[http://seananderson.ca/ggplot2-FISH554/][nice ggplot2 tutorial]].

Hadley Wickam provides a [[https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html][nice tour of dplyr]] and [[http://blog.rstudio.org/2014/07/22/introducing-tidyr/][gentle introduction to
tidyR]]. Here is a nice link on [[https://stat545-ubc.github.io/bit001_dplyr-cheatsheet.html][merging data frames]].

The Rstudio team has designed a [[https://www.rstudio.com/resources/cheatsheets/][nice series of cheatsheets on R]] and in
particular one on [[https://www.rstudio.com/wp-content/uploads/2015/05/ggplot2-cheatsheet.pdf][ggplot2]] and on [[https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf][R/markdown/knitr]].
* References
+ R. Jain, [[http://www.cs.wustl.edu/~jain/books/perfbook.htm][The Art of Computer Systems Performance Analysis:
  Techniques for Experimental Design, Measurement, Simulation, and
  Modeling]], Wiley-Interscience, New York, NY, April 1991.
  [[http://www.amazon.com/Art-Computer-Systems-Performance-Analysis/dp/1118858425/ref%3Dsr_1_2?s%3Dbooks&ie%3DUTF8&qid%3D1435137636&sr%3D1-2&keywords%3Dperformance%2Bmeasurement%2Bcomputer][A new edition will be available in September 2015]].
  #+BEGIN_QUOTE
  This is an easy-to-read self-content book for practical performance
  evaluation. The numerous checklists make it a great book for
  engineers and every CS experimental scientist should have read it.
  #+END_QUOTE
+ David J. Lilja, Measuring Computer Performance: A Practitioner’s
  Guide, Cambridge University Press 2005
  #+BEGIN_QUOTE
  A short book suited for brief presentations. I follow a similar
  organization but I really don't like the content of this book. I
  feel it provides very little insight on why the theory applies or
  not. I also think it is too general and lacks practical examples. It
  may be interesting for those willing a quick and broad presentation
  of the main concepts and "recipes" to apply.
  #+END_QUOTE
+ Jean-Yves Le Boudec. [[http://www.cl.cam.ac.uk/~dq209/others/perf.pdf][Methods, practice and theory for the
  performance evaluation of computer and communication
  systems, 2006. EPFL electronic book]].
  #+BEGIN_QUOTE
  A very good book, with a much more theoretical treatment than the
  Jain. It goes way farther on many aspects and I can only recommand
  it.
  #+END_QUOTE
+ Douglas C. Montgomery, [[http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002024.html][Design and Analysis of Experiments]], 8th
  Edition. Wiley 2013.
  #+BEGIN_QUOTE
  This is a good and thorough textbook on design of experiments. It's
  so unfortunate it relies on "exotic" softwares like JMP and minitab
  instead of R...
  #+END_QUOTE
+ Julian J. Faraway, [[https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf][Practical Regression and Anova using R]],
  University of Bath, 2002.
  #+BEGIN_QUOTE
  This book is derived from material that Pr. Faraway used in a Master
  level class on Statistics at the University of Michigan. It is
  mathematically involved but presents in details how linear
  regression, ANOVA work and can be done with R. It works out many
  examples in details and is very pleasant to read. A must-read if you
  want to understand this topic more thoroughly.
  #+END_QUOTE
+ Peter Kosso, [[http://www.amazon.fr/Summary-Scientific-Method-Peter-Kosso-ebook/dp/B008D5IYU2][A Summary of Scientific Method]], Springer, 2011. [[[http://hemija.pmf.ukim.edu.mk/materials/download/6d31fd3f53a82da9de163833806722ae][hidden
  PDF that google found on the webpage of a university in Macedonia]]
  #+BEGIN_QUOTE
  A short nice book summarizing the main steps of the scientific
  method and why having a clear definition is not that simple. It
  illustrates these points with several nice historical examples that
  allow the reader to take some perspective on this epistemological
  question.
  #+END_QUOTE
+ R. Nelson, Probability stochastic processes and queuing theory: the
  mathematics of computer performance modeling. Springer Verlag 1995.
  #+BEGIN_QUOTE
  For those willing to know more about queuing theory.
  #+END_QUOTE
