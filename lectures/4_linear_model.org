#+AUTHOR:      Arnaud Legrand
#+TITLE:       Introduction to Linear Regression
#+DATE:        Performance Evaluation Lecture
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+STARTUP: beamer overview indent
#+TAGS: noexport(n)
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+LATEX_HEADER: \input{org-babel-style-preembule.tex}

#+LaTeX: \input{org-babel-document-preembule.tex}

* List                                                             :noexport:
** TODO Talk about extensions
krigging ?? generalized linear model ?
* Documents 							   :noexport:
** [[/home/alegrand/Archives/Cours/maths/ModeleLineaireRegrDegerine.pdf][Régression Linéaire]]
  http://en.wikipedia.org/wiki/Simple_linear_regression
  http://en.wikipedia.org/wiki/Linear_regression_model

** Generalized Linear model
http://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf

http://data.princeton.edu/wws509/notes/a2.pdf
  
* Simple Linear Regression
** General Introduction
*** What is a regression?
    Regression analysis is the most widely used statistical tool for
    *understanding relationships among variables*. Several possible
    objectives including:
    1. *Prediction* of future observations. This includes
       extrapolation since we all like connecting points by lines when
       we /expect/ things to be continuous
    2. Assessment of the *effect* of, or *relationship* between,
       explanatory variables on the response
    3. A *general description* of data structure (generally expressed
       in the form of *an equation or a model* connecting the response
       or dependent variable and one or more explanatory or predictor
       variable)
    4. Defining what you should "expect" as it allows you to define
       and detect what *does not behave as expected*
    
    The linear relationship is the most commonly found one
    - we will illustrate how it works
    - it is very general and is the basis of many more advanced tools
      (polynomial regression, ANOVA, ...)
** Fitting a Line to a Set of Points
*** Starting With a Simple Data Set
    *Descriptive statistics* provides simple summaries about the
    sample and about the observations that have been made.
    #+BEGIN_CENTER
       How could we summarize the following data set ?
    #+END_CENTER

#+BEGIN_SRC R :session :results none :exports none
library(ggplot2)
x=c(1, 2, 3, 5, 7, 9, 11, 13, 14, 15)
y=c(3, 1, 5, 2, 6, 4, 7, 9, 8, 5)
df=data.frame(x=x,y=y)
df_lines=data.frame(
          slope=c(.37,.67,.57,.41,.50,.385),intercept=c(2.0,-0.4,0.4,1.7,1.0,2.6),
          label=c("Least Squares \n (y~x)","Least Squares (x~y)", "Least Squares",
                 "Least Distance", "Least Rectangles", "Eyeball"),
          posx=c(16,15,13,5,3,8),
          posy=c(4.5,10,8,4,1,6),
          hjust=c(1,1,1,1,0,1),
          color=c("Least Squares","Least Squares","Least Squares x^2+y^2",
                  "Least Distance","Least Rectangles","Eyeball"))
#lm(data=df,y~x)
#lm(data=df,x~y)
#+END_SRC

#+LaTeX: \begin{tabular}[c]{cc} \fbox{

  #+BEGIN_SRC R :session :results output latex :exports results
  library(xtable)
  print(xtable(df),floating=FALSE)
  #+END_SRC

  #+RESULTS:
  #+BEGIN_EXPORT latex
  % latex table generated in R 3.0.2 by xtable 1.7-1 package
  % Thu Oct 31 13:30:32 2013
  \begin{tabular}{rrr}
    \hline
   & x & y \\ 
    \hline
  1 & 1.00 & 3.00 \\ 
    2 & 2.00 & 1.00 \\ 
    3 & 3.00 & 5.00 \\ 
    4 & 5.00 & 2.00 \\ 
    5 & 7.00 & 6.00 \\ 
    6 & 9.00 & 4.00 \\ 
    7 & 11.00 & 7.00 \\ 
    8 & 13.00 & 9.00 \\ 
    9 & 14.00 & 8.00 \\ 
    10 & 15.00 & 5.00 \\ 
     \hline
  \end{tabular}
  #+END_EXPORT

#+LaTeX: }& \hspace{1cm}\raisebox{-.5\height}{\resizebox{.5\linewidth}{!}{

  #+BEGIN_SRC R :session :exports none :results output graphics :file "./pdf_babel/scatterplot.pdf" :width 3 :height 3
  ggplot() + geom_point(data=df,aes(x=x,y=y),color="blue") + 
    xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw()
  #+END_SRC

  #+RESULTS:
  [[file:./pdf_babel/scatterplot.pdf]]

  #+ATTR_LATEX: :width \linewidth :center nil
  [[file:./pdf_babel/scatterplot.pdf]]

#+LaTeX: }}\end{tabular}

*** The "Eyeball" Method
    \vspace{-1em}
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_eyeball.pdf" :width 3 :height 3
    ggplot() + geom_point(data=df,aes(x=x,y=y),color="blue") + 
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw() +
       geom_abline(data=df_lines[df_lines$label=="Eyeball",], aes(intercept=intercept,slope=slope),color="red")
    #+END_SRC

    #+BEGIN_CENTER
    #+ATTR_LATEX: :height 5cm
    [[file:./pdf_babel/scatterplot_eyeball.pdf]]
    \vspace{-1em}
    #+END_CENTER

    - A straight line drawn through the maximum number of points on a
      scatter plot balancing about an equal number of points above and
      below the line
    - Some points are rather far from the line. Maybe we should instead
      try to minimize some kind of /distance to the line/
*** Least Squares Line (1): What to minimize?
    \vspace{-1em}
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_leastsquares.pdf" :width 3 :height 3
    intercept_eyeball = df_lines[df_lines$label=="Eyeball",]$intercept
    slope_eyeball = df_lines[df_lines$label=="Eyeball",]$slope
    ggplot() + 
       geom_segment(data=df, aes(x=x, y=y, xend=x, yend=x*slope_eyeball+intercept_eyeball)) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw() +
       geom_abline(data=df_lines[df_lines$label=="Eyeball",], aes(intercept=intercept,slope=slope),color="red")
    #+END_SRC

    #+BEGIN_CENTER
    #+ATTR_LATEX: :height 5cm
    [[file:./pdf_babel/scatterplot_leastsquares.pdf]]
    #+END_CENTER

    \vspace{-1em}
    Intuitively, a large error is /much/ more important than a small
    one. We could try to minimize
    $\displaystyle F(\alpha,\beta) = \sum_{i}
    {\underbrace{\left(y_i - \alpha - \beta x_i \right)}_{e_i}}^2$,
    the size of *all* residuals:
    - If they were all zero we would have a perfect line
    - Trade-off between moving closer to some points and at the same
      time moving away from other points
    \vspace{-1em}
*** Least Squares Line (2): Simple Formula
    \vspace{-1em}
    #+BEGIN_EXPORT latex
    $$F(\alpha,\beta) = \sum_{i=1}^n \left(y_i - \alpha - \beta x_i \right)^2$$
    #+END_EXPORT
    $F$ is quadratic in \alpha and in \beta so if we simply
    differentiate $F$ by \alpha and by \beta, we can obtain a closed
    form for the minimum:
    #+BEGIN_EXPORT latex
    \begin{align*} 
      \hat\beta & = \frac{ \sum_{i=1}^{n} (x_{i}-\bar{x})(y_{i}-\bar{y}) }{ \sum_{i=1}^{n} (x_{i}-\bar{x})^2 } 
       = \frac{ \sum_{i=1}^{n}{x_{i}y_{i}} - \frac1n \sum_{i=1}^{n}{x_{i}}\sum_{j=1}^{n}{y_{j}}}{ \sum_{i=1}^{n}({x_{i}^2}) - \frac1n (\sum_{i=1}^{n}{x_{i}})^2 } \\[6pt] 
     & = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 } = \frac{ \operatorname{Cov}[x,y] }{ \operatorname{Var}[x] } = r_{xy} \frac{s_y}{s_x} \\[6pt] 
     \hat\alpha  & = \bar{y} - \hat\beta\,\bar{x} \text{, where:}
    \end{align*}
    #+END_EXPORT
    \vspace{-2em}
    - $\bar x$ and $\bar y$ are the sample mean of $x$ and $y$
    - $r_{xy}$ is the sample correlation coefficient between $x$ and $y$
    - $s_x$ and $s_y$ are the sample standard deviation of $x$ and $y$
    Also it has a good geometric interpretation (*orthogonal projection*)
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_leastsquares2.pdf" :width 3 :height 3
    intercept_lsx = df_lines[df_lines$label=="Least Squares \n (y~x)",]$intercept
    slope_lsx = df_lines[df_lines$label=="Least Squares \n (y~x)",]$slope
    ggplot() + 
       geom_segment(data=df, aes(x=x, y=y, xend=x, yend=x*slope_lsx+intercept_lsx)) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw() +
       geom_abline(data=df_lines[df_lines$label=="Least Squares \n (y~x)",], aes(intercept=intercept,slope=slope),color="red") +
       geom_abline(data=df_lines[df_lines$label=="Eyeball",], aes(intercept=intercept,slope=slope),color="gray")
    #+END_SRC
*** Least Squares Line (3): y as a function of x or the opposite?

    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_leastsquares3.pdf" :width 3 :height 3
    intercept_lsy = df_lines[df_lines$label=="Least Squares (x~y)",]$intercept
    slope_lsy = df_lines[df_lines$label=="Least Squares (x~y)",]$slope
    ggplot() + 
       geom_segment(data=df, aes(x=x, xend=(df$y-intercept_lsy)/slope_lsy, y=y, yend=y)) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       geom_abline(data=df_lines[df_lines$label=="Least Squares \n (y~x)",], aes(intercept=intercept,slope=slope),color="gray") +
       geom_abline(data=df_lines[df_lines$label=="Least Squares (x~y)",], aes(intercept=intercept,slope=slope),color="red") +
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw()
    #+END_SRC


    #+BEGIN_CENTER
    #+LaTeX: \only<1>{%
      #+ATTR_LATEX: :height 5cm
      [[file:./pdf_babel/scatterplot_leastsquares2.pdf]]
    #+LaTeX: }\only<2->{%
      #+ATTR_LATEX: :height 5cm
      [[file:./pdf_babel/scatterplot_leastsquares3.pdf]]
    #+LaTeX: }
    #+END_CENTER

    #+LaTeX: \uncover<3>{\centerline{OK, do we have less asymetrical options?}}\vspace{10cm}
*** Least Distances Line (a.k.a. Deming Regression)
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_deming.pdf" :width 3 :height 3
    intercept_deming = df_lines[df_lines$label=="Least Distance",]$intercept
    slope_deming = df_lines[df_lines$label=="Least Distance",]$slope
    u = c(1,slope_deming)
    u = u/(sqrt(u[1]^2+u[2]^2))
    
    df$d = (df$x*u[1]+(df$y - intercept_deming)*u[2])

    df$xend = df$d*u[1]
    df$yend = df$d*u[2] + intercept_deming

    ggplot() + 
       geom_segment(data=df, aes(x=x, y=y, xend=xend, yend=yend)) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       geom_abline(data=df_lines[df_lines$label=="Least Distance",], aes(intercept=intercept,slope=slope),color="red") +
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw()
    #+END_SRC

    #+BEGIN_CENTER
      #+ATTR_LATEX: :height 5cm
      [[file:./pdf_babel/scatterplot_deming.pdf]]
    #+END_CENTER

    - Note that somehow, this makes sense only if we have a square
      plot, i.e., if $x$ and $y$ have the same units
    #+LaTeX: \vspace{10cm}
*** Least Rectangles Line
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_rectangle.pdf" :width 3 :height 3
    intercept_rect = df_lines[df_lines$label=="Least Rectangles",]$intercept
    slope_rect = df_lines[df_lines$label=="Least Rectangles",]$slope

    ggplot() + 
       geom_rect(data=df, aes(xmin=x, ymin=y, ymax= intercept_rect + slope_rect*x, xmax= (y- intercept_rect)/slope_rect),alpha=.3) +
       geom_segment(data=df, aes(x=x, xend=(df$y-intercept_rect)/slope_rect, y=y, yend=y),linetype=2) +
       geom_segment(data=df, aes(x=x, y=y, xend=x, yend=x*slope_rect+intercept_rect),linetype=2) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       geom_abline(data=df_lines[df_lines$label=="Least Rectangles",], aes(intercept=intercept,slope=slope),color="red") +
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw()
    #+END_SRC

    #+BEGIN_CENTER
      #+ATTR_LATEX: :height 5cm
      [[file:./pdf_babel/scatterplot_rectangle.pdf]]
    #+END_CENTER

    - Minimize $E(\alpha,\beta) = \sum_{i=1}^n
      \left|x_i-\frac{y_i-\alpha}{\beta}\right|\cdot\left|y_i-\alpha-\beta
      x_i\right|$
    - This leads to the regression line
      $y=\frac{s_y}{s_x}(x-\bar{x}) + \bar{y}$.
    #+LaTeX: \vspace{10cm}
*** Least Squares (in Both Directions) Line
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/scatterplot_squares.pdf" :width 3 :height 3
    intercept_ls = df_lines[df_lines$label=="Least Squares",]$intercept
    slope_ls = df_lines[df_lines$label=="Least Squares",]$slope

    ggplot() + 
       geom_segment(data=df, aes(x=x, xend=(df$y-intercept_ls)/slope_ls, y=y, yend=y)) +
       geom_segment(data=df, aes(x=x, y=y, xend=x, yend=x*slope_ls+intercept_ls)) +
       geom_point(data=df,aes(x=x,y=y),color="blue") + 
       geom_abline(data=df_lines[df_lines$label=="Least Squares",], aes(intercept=intercept,slope=slope),color="red") +
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw()
    #+END_SRC

    #+BEGIN_CENTER
      #+ATTR_LATEX: :height 5cm
      [[file:./pdf_babel/scatterplot_squares.pdf]]
    #+END_CENTER

    - Minimize $D(\alpha,\beta) = \sum_{i=1}^n
      \left(x_i-\frac{y_i-\alpha}{\beta}\right)^2 + 
      \left(y_i-\alpha-\beta x_i\right)^2$
    - Has to be computed analytically
    #+LaTeX: \vspace{10cm}
*** Which line to choose?
#   http://i.imgur.com/egd5f.png
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/linear_regressions.pdf" :width 7 :height 5
     ggplot() + geom_abline(data=df_lines,aes(intercept=intercept,slope=slope,color=color),size=.8) + 
       geom_point(data=df,aes(x=x,y=y),color="blue") + scale_colour_brewer(palette="Set1") +
       xlim(0,16)+ylim(0,16)+ coord_fixed() + theme_bw() + labs(color="Regression type") +
       geom_text(data=df_lines,aes(x=posx,y=posy,label=label,hjust=hjust,color=color)) 
    #+END_SRC

    #+BEGIN_CENTER
      #+ATTR_LATEX: :width .9\linewidth
      [[file:./pdf_babel/linear_regressions.pdf]]
    #+END_CENTER
    #+LaTeX: \vspace{10cm}
*** What does correspond to each line?
    - Eyeball: AFAIK nothing
    - Least Squares: classical linear regression $y\sim x$
    - Least Squares in both directions: I don't know
    - Deming: equivalent to Principal Component Analysis
    - Rectangles: may be used when one variable is not "explained"
      by the other, but are inter-dependent

    This is not just a geometric problem. You need a *model* of to
    decide which one to use
* Linear Model
** Linear Regression
*** The Simple Linear Regression Model
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/regression.pdf" :width 4 :height 5
    x=runif(50,min=-20,max=60)
    a=5
    b=.5
    y=a+b*x+rnorm(50,sd=3)
    df = data.frame(x=x,y=y)
    ggplot() + theme_bw() + geom_hline(yintercept=0) + geom_vline(xintercept=0) +
       geom_abline(intercept=a,slope=b,color="red",size=1) + 
       geom_point(data=df,aes(x=x,y=y),color="blue") 
    #+END_SRC

#+LaTeX: \vspace{-1em}
#+LaTeX:   \begin{columns}
#+LaTeX:     \begin{column}{.62\linewidth}
    We need to invest in a *probability model*
    #+LaTeX: \centerline{$\displaystyle \rv{Y} = a + b X + \rv{\epsilon}$}
    
    - \rv{Y} is the *response variable*
      #+LaTeX: \item$X$~is~a~continuous~explanatory~variable
    - $a$ is the intercept
    - $b$ is the slope
    - \rv{\epsilon} is some noise
#+LaTeX:     \end{column}\hspace{-2em}
#+LaTeX:     \begin{column}{.3\linewidth}
      #+ATTR_LATEX: :width \linewidth
      [[file:./pdf_babel/regression.pdf]]
#+LaTeX:     \end{column}
#+LaTeX:   \end{columns}

    - $a + b X$ represents the “true line”, the part of
      \rv{Y} that depends on $X$
    - The error term \rv{\epsilon} is independent “idosyncratic noise”,
      \ie the part of \rv{Y} not associated with $X$
    \pause
****** Gauss-Markov Theorem
Under a few assumptions, the least squares regression is the Best
Linear Unbiased Estimate
+ $\E[\rv{\hat\beta}] = b$ and $\E[\rv{\hat\alpha}] = a$ 
  #+LaTeX: \hfil \usebeamertemplate{itemize item}
   $\Var(\rv{\hat\beta})$ and $\Var(\rv{\hat\alpha})$ are minimal\hfil
*** Multiple explanatory variables
    - The same results hold true when there are *several* explanatory
      variables:\smallskip

      #+BEGIN_EXPORT latex
      \centerline{$\rv{Y} = a + b^{(1)}X^{(1)} + b^{(2)}X^{(2)} +
        b^{(1,2)}X^{(1)}X^{(2)} + \rv{\epsilon} $}
      #+END_EXPORT
      \smallskip

      The least squares regressions are good estimators of $a$,
      $b^{(1)}$, $b^{(2)}$, $b^{(1,2)}$ \medskip
    - We can use an *arbitrary* linear combination of variables,
      hence \smallskip

      #+BEGIN_EXPORT latex
      \centerline{$\rv{Y} = a + b^{(1)}X + b^{(2)}\frac{1}{X} + b^{(3)}X^3 +
      \rv{\epsilon} $}    
      #+END_EXPORT
      \smallskip
      is also a linear model
    - Obviously the closed-form formula are much more complicated but
      softwares like *R* handle this very well
** Underlying Hypothesis
*** Important Hypothesis (1)
    - Weak exogeneity :: The predictor variables $X$ can be treated as
         fixed values, rather than random variables: the $X$ are
         assumed to be *error-free*, i.e., they are not contaminated
         with measurement errors

         Although not realistic in many settings, dropping this
         assumption leads to significantly more difficult
         /errors-in-variables/ models
    - Linearity ::  the mean of the response variable is a linear
                    combination of the parameters (regression
                    coefficients) and the predictor variables

		    Since predictor variables themselves can be
                    arbitrarily transformed, this is not that
                    restrictive. This trick is used, for example, in
                    *polynomial regression*, but beware of
                    *overfitting*
    - Independance of Errors :: if several responses $\rv{Y_1}$ and
         $\rv{Y_2}$ are fit, $\rv{\epsilon_1}$ and $\rv{\epsilon_2}$ should be
         independant
*** Other Very Important Hypothesis
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/homoscedasticity.pdf" :width 6 :height 3
    x=runif(50,min=-20,max=60)
    a=5
    b=.5
    y=a+b*x+rnorm(50,sd=2)
    df = data.frame(x=x,y=y,type="homoscedastic")
    y=a+(b)*x + rnorm(50,sd=.15)*(x+20)
    df = rbind(df,data.frame(x=x,y=y,type="heteroscedastic"))
    y=x^2/60+a+rnorm(50,sd=2)
    df = rbind(df,data.frame(x=x,y=y,type="quadratic"))

    ggplot(data=df[df$type!="quadratic",]) + theme_bw() + geom_hline(yintercept=0) + geom_vline(xintercept=0) +
       geom_abline(intercept=a,slope=b,color="red",size=1) + 
       geom_point(aes(x=x,y=y),color="blue") +
       facet_wrap(~type)
    #+END_SRC

    - Constant variance (a.k.a. homoscedasticity) :: 
	 #+LaTeX: ~
         #+BEGIN_CENTER
         #+ATTR_LATEX: :width \linewidth
         [[file:./pdf_babel/homoscedasticity.pdf]]
         #+END_CENTER
	 \vspace{-2em}
      - Variance is independant of $X$
      - If several responses $\rv{Y_1}$ and $\rv{Y_2}$ are fit,
        $\rv{\epsilon_1}$ and $\rv{\epsilon_2}$ should have the same variance
      - Either normalize $Y$ or use an other estimator
*** Dummy							   :noexport:
    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/data_sets.pdf" :width 8 :height 3
    ggplot(data=df,aes(x=x,y=y)) + theme_bw() + geom_hline(yintercept=0) + geom_vline(xintercept=0) +
       geom_smooth(method='lm',color="red") +
       geom_point(,color="blue") +
       facet_wrap(~type)
    #+END_SRC
*** Other Classical Hypothesis (3)
    - Normal and iid errors :: This is *not* an assumption of the
         Gauss Markov Theorem. Yet, it is quite convenient to build
         /precise/ confidence intervals of the regression
    - Arrangement of the predictor variables $X$ :: it has a major
         influence on the precision of estimates of $\beta$ (remember
         Anscombe's quartet). 
          #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/anscombe.pdf" :width 8 :height 3
          library(reshape)
          anscombe$idx=1:length(anscombe$x1) 
          a = melt(anscombe,id=c("idx"))
          a$set=gsub("[^0-9]*","",as.character(a$variable))
          a$variable=gsub("[0-9]*","",as.character(a$variable))
          a = cast(a, idx+set~variable, mean) 
          ggplot(data=a,aes(x=x,y=y)) + theme_bw() +
             geom_point(color="blue") + facet_wrap(~set,nrow=1) + geom_smooth(method='lm',color="red")
          #+END_SRC
         #+BEGIN_CENTER
         #+ATTR_LATEX: :width \linewidth
         [[file:./pdf_babel/anscombe.pdf]]
         #+END_CENTER
	 \vspace{-1em}
      This is part of your design of experiments:
      - If you want to test linearity, $X$ should be uniformly
        distributed
      - If you want the best estimation, you should use extreme values
        of $X$
** Checking hypothesis
*** Linearity: Residuals vs. Explanatory Variable
   #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/linearity_check.pdf" :width 6 :height 3
   lm_hom=lm(data=df[df$type=="homoscedastic",],y~x)
   lm_hom=fortify(lm_hom)
   lm_hom$type="homoscedastic"

   lm_het=lm(data=df[df$type=="heteroscedastic",],y~x)
   lm_het=fortify(lm_het)
   lm_het$type="heteroscedastic"

   lm_quad=lm(data=df[df$type=="quadratic",],y~x)
   lm_quad=fortify(lm_quad)
   lm_quad$type="quadratic"

   lm_both = rbind(lm_hom,lm_quad)
   lm_both$type <- factor(lm_both$type, levels=c("homoscedastic","quadratic"))
   ggplot(data=lm_both,aes(x,.resid)) + theme_bw() +
     geom_smooth(method='lm',color="red",se=F) +
     geom_point() + facet_wrap(~type)
   #+END_SRC

	#+BEGIN_CENTER
	#+ATTR_LATEX: :width .9\linewidth
	[[file:./pdf_babel/linearity_check.pdf]]
	#+END_CENTER

    When there are several factors, you have to check for every
    dimension... \frowny That's why one generally study residuals as a
    function of fitted values.
    \vspace{10cm}
*** Homoscedasticity: Residuals vs. Fitted values
   #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/homoscedasticity_check.pdf" :width 6 :height 3
   lm_both = rbind(lm_het,lm_hom)
   lm_both$type <- factor(lm_both$type, levels=c("homoscedastic","heteroscedastic"))
   ggplot(data=lm_both,aes(.fitted,.resid)) + theme_bw() +
     geom_smooth(method='lm',color="red",se=F) +
     geom_point() + facet_wrap(~type)
   #+END_SRC

	#+BEGIN_CENTER
	#+ATTR_LATEX: :width .9\linewidth
	[[file:./pdf_babel/homoscedasticity_check.pdf]]
	#+END_CENTER

    \vspace{10cm}
*** Normality: qqplots
   #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/normality_check.pdf" :width 6 :height 3
   qplot.data <- function (vec)  { # argument: vector of numbers
     # following four lines from base R's qqline()
     y <- quantile(vec[!is.na(vec)], c(0.25, 0.75))
     x <- qnorm(c(0.25, 0.75))
     slope <- diff(y)/diff(x)
     int <- y[1L] - slope * x[1L]
     d <- data.frame(slope=slope, int=int)
     d
   }
   
   res_n = data.frame(sample=rnorm(50))
   res_qplot=qplot.data(res_n$sample)
   res_n$type = "normal"
   res_qplot$type = "normal"

   res_u = data.frame(sample=rexp(50))
   res_u$type = "exponential"
   res_qplotu=qplot.data(res_u$sample)
   res_qplotu$type = "exponential"

   res_qplot = rbind(res_qplot,res_qplotu)
   res_both = rbind(res_n,res_u)

   res_both$type <- factor(res_both$type, levels=c("normal","exponential"))
   res_qplot$type <- factor(res_qplot$type, levels=c("normal","exponential"))

   ggplot() + theme_bw() +
     stat_qq(data=res_both,aes(sample=sample)) + 
     geom_abline(data=res_qplot,aes(slope=slope,intercept=int)) + 
     facet_wrap(~type,scale="free_y")
   #+END_SRC

	#+BEGIN_CENTER
	#+ATTR_LATEX: :width .9\linewidth
	[[file:./pdf_babel/normality_check.pdf]]
	#+END_CENTER
    A quantile-quantile plot is a graphical method for comparing two
    probability distributions by plotting their quantiles against each
    other

    \vspace{10cm}
*** Model Formulae in R
    The structure of a model is specified in the formula like this:
    #+BEGIN_CENTER
    *response variable* =~= *explanatory variable(s)*
    #+END_CENTER
    =~= reads "is modeled as a function of " and
    =lm(y~x)= means $\rv{y} = a + b x + \rv{\epsilon}$ \medskip

    On the right-hand side, one should specify how the explanatory
    variables are combined. The symbols used here have a *different
    meaning* than in arithmetic expressions
    - =+= indicates a variable inclusion (not an addition)
    - =-= indicates a variable deletion (not a substraction)
    - =*= indicates inclusion of variables and their interactions
    - =:= means an interaction
    
    Therefore
    - =z~x+y= means $\rv{z} = a + b_1 x + b_2 y + \rv{\epsilon}$
    - =z~x*y= means $\rv{z} = \alpha + b_1 x + b_2 y + b_3 xy +
      \rv{\epsilon}$
    - =z~(x+y)^2= means the same
    - =log(y)~I(1/x)+x+I(x^2)= means $\rv{z} = \alpha + b_1
      \frac{1}{x} + b_2 x + b_3 x^2 + \rv{\epsilon}$
*** Checking the model with R
     \small
     #+BEGIN_SRC R :session :exports code :results none graphics :file "./pdf_babel/model_standardcheck.pdf" :width 5 :height 5
     reg <- lm(data=df[df$type=="heteroscedastic",],y~x)
     par(mfrow=c(2,2));   plot(reg);   par(mfrow=c(1,1))
     #+END_SRC
**** 
     #+LaTeX: %\vspace{-2em}
    #+BEGIN_CENTER
      #+ATTR_LATEX: :height 6.5cm
      [[file:./pdf_babel/model_standardcheck.pdf]] 
    #+END_CENTER
     #+LaTeX: %\vspace{-2em}
** Decomposing the Variance
*** Decomposing the Variance
    #+BEGIN_CENTER
      *How well does the least squares line explain variation in $Y$?*
    #+END_CENTER

    #+BEGIN_SRC R :session :exports none :results none graphics :file "./pdf_babel/linear_regression2.pdf" :width 6 :height 3
    ggplot(data=df[df$type=="homoscedastic",],aes(x=x,y=y)) + theme_bw() + geom_hline(yintercept=0) + geom_vline(xintercept=0) +
       geom_smooth(method='lm',color="red",size=1,se=F) + 
       geom_point(color="blue") 
    #+END_SRC

    #+LaTeX:  \only<+>{
	#+BEGIN_CENTER
	#+ATTR_LATEX: :width .9\linewidth
	[[file:./pdf_babel/linear_regression2.pdf]]
	#+END_CENTER
    #+LaTeX: }\uncover<+->{
    
    We have $\rv{Y}=\hat{Y}(\rv{X})+\rv{\epsilon}$
    \hfill
    ($\hat{Y}$ is the "true mean"; we note $\rv{\hat{Y}}=\hat{Y}(\rv{X})$).

    Since $\rv{\hat{Y}}$ and $\rv{\epsilon}$ are uncorrelated, we have
    #+BEGIN_EXPORT latex
    \begin{align*}
      \Var(\rv{Y}) & = \Var(\rv{\hat{Y}}+\rv{\epsilon}) = \Var(\rv{\hat{Y}}) + \Var(\rv{\epsilon}) \\
      \frac{1}{n-1}\sum_{i=1}^n (\rv{Y_i} - \rv{\bar{Y}})^2 & =
      \frac{1}{n-1}\sum_{i=1}^n (\rv{\hat{Y}_i} - \rv{\overline{\hat{Y}}})^2 + 
      \frac{1}{n-1}\sum_{i=1}^n (\rv{\epsilon_i} - \rv{\bar{\epsilon}})^2\\[-.7em]
    \intertext{Since $\rv{\bar{\epsilon}} = 0$ and $\rv{\bar{Y}} = \rv{\overline{\hat{Y}}}$, we have\vspace{-.7em}}
      \underbrace{\sum_{i=1}^n (\rv{Y_i} - \rv{\bar{Y}})^2}_{\text{Total Sum of Squares}} & =
      \underbrace{\sum_{i=1}^n (\rv{\hat{Y}_i} - \rv{\bar{Y}})^2}_{\text{Regression SS}} + 
      \underbrace{\sum_{i=1}^n \rv{\epsilon_i}^2}_{\text{Error SS}}
    \end{align*}
    #+END_EXPORT

    - $SSR$ = Variation in $\rv{Y}$ explained by the regression line
    - $SSE$ = Variation in $\rv{Y}$ that is left unexplained

    #+BEGIN_CENTER
        *$SSR$ = $SST$ $\Rightarrow$ perfect fit*
    #+END_CENTER
    #+LaTeX: }
*** A Goodness of Fit Measure: \RR
    The *coefficient of determination*, denoted by *$R^2$*, measures
    goodness of fit:
    #+BEGIN_CENTER
    #+BEGIN_EXPORT latex
    $\displaystyle R^2= 1-\frac{SSE}{SST} = 
     1 - \frac{\sum_{i=1}^n (y_i - \hat{y}(x_i))^2}{\sum_{i=1}^n (y_i -
       \bar{y})^2} =
     1 - \frac{\text{\alert{the error knowing $x$}}}{\text{\alert{the error without knowing $x$}}}
     $
    #+END_EXPORT
    #+END_CENTER
    - $0\leq R^2 \leq 1$
    - The closer $R^2$ is to $1$, the better the fit

    Warning:
    - A not so low $R^2$ may mean important noise or bad model
      - In biology or social sciences, an $R^2$ of .6 can be
        considered as good
      - In physics/engineering, an $R^2$ of .6 would be considered as low
    - As you add parameters to a model, you inevitably improve the
      fit
      - The *adjusted $R^2$* tries to compensate this
      - There is a trade-off beteween model simplicity and fit. Strive
        for simplicity!
*** Illustration with R (homoscedastic data)
    #+LaTeX: \only<+>{
      #+ATTR_LATEX: :width \linewidth
      [[file:./pdf_babel/data_sets.pdf]]
    #+LaTeX: }
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    #+BEGIN_SRC R :session :exports both :results output
    reg <- lm(data=df[df$type=="homoscedastic",],y~x)
    summary(reg)
    #+END_SRC
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    \scriptsize
    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = y ~ x, data = df[df$type == "homoscedastic", ])

    Residuals:
	Min      1Q  Median      3Q     Max 
    -4.1248 -1.3059 -0.0366  1.0588  3.9965 

    Coefficients:
		Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  4.56481    0.33165   13.76   <2e-16 ***
    x            0.50645    0.01154   43.89   <2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 1.816 on 48 degrees of freedom
    Multiple R-squared:  0.9757,	Adjusted R-squared:  0.9752 
    F-statistic:  1926 on 1 and 48 DF,  p-value: < 2.2e-16
#+end_example
**** 
    :PROPERTIES:
    :BEAMER_act: <+>
    :END:
#+LaTeX: \begin{overlayarea}{\linewidth}{0cm}\vspace{-8.8cm}\begin{block}{}


- =Std. Error= = $\sigma/\sqrt{n}$ and can be used are used to compute C.I on
  the regression estimates
- =t-value= and =Pr(>|t|)=: $t$-test whether $\mu\neq0$
  - Easy to read significance codes
  - Assumes normality
- =F-statistic=: test the null hypothesis that all of the model
  coefficients are 0
#+LaTeX: \end{block}\end{overlayarea}

*** Illustration with R (heteroscedastic data)
    #+LaTeX: \only<+>{
      #+ATTR_LATEX: :width \linewidth
      [[file:./pdf_babel/data_sets.pdf]]
    #+LaTeX: }
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    #+BEGIN_SRC R :session :exports both :results output
    reg <- lm(data=df[df$type=="heteroscedastic",],y~x)
    summary(reg)
    #+END_SRC
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    \scriptsize
    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = y ~ x, data = df[df$type == "heteroscedastic", ])

    Residuals:
	Min      1Q  Median      3Q     Max 
    -25.063  -3.472   0.663   3.707  19.327 

    Coefficients:
		Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  4.98800    1.41061   3.536 0.000911 ***
    x            0.56002    0.04908  11.411 2.83e-15 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 7.722 on 48 degrees of freedom
    Multiple R-squared:  0.7306,	Adjusted R-squared:  0.725 
    F-statistic: 130.2 on 1 and 48 DF,  p-value: 2.83e-15
#+end_example

*** Illustration with R (quadratic data)
    #+LaTeX: \only<+>{
      #+ATTR_LATEX: :width \linewidth
      [[file:./pdf_babel/data_sets.pdf]]
    #+LaTeX: }
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    #+BEGIN_SRC R :session :exports both :results output
    reg <- lm(data=df[df$type=="quadratic",],y~x)
    summary(reg)
    #+END_SRC
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    \scriptsize
    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = y ~ x, data = df[df$type == "quadratic", ])

    Residuals:
	Min      1Q  Median      3Q     Max 
    -11.759  -5.847  -2.227   3.746  17.346 

    Coefficients:
		Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  7.05330    1.41238   4.994 8.23e-06 ***
    x            0.65517    0.04914  13.333  < 2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 7.732 on 48 degrees of freedom
    Multiple R-squared:  0.7874,	Adjusted R-squared:  0.783 
    F-statistic: 177.8 on 1 and 48 DF,  p-value: < 2.2e-16
#+end_example

*** Illustration with R (quadratic data, polynomial regression)
    #+LaTeX: \only<+>{
      #+ATTR_LATEX: :width \linewidth
      [[file:./pdf_babel/data_sets.pdf]]
    #+LaTeX: }
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    #+BEGIN_SRC R :session :exports both :results output
    df$x2=df$x^2
    reg_quad <- lm(data=df[df$type=="quadratic",],y~x+x2)
    summary(reg_quad)
    #+END_SRC
**** 
    :PROPERTIES:
    :BEAMER_act: <+->
    :END:
    \scriptsize
    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = y ~ x + x2, data = df[df$type == "quadratic", ])

    Residuals:
	Min      1Q  Median      3Q     Max 
    -4.7834 -0.8638 -0.0480  1.1312  3.9913 

    Coefficients:
		 Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 5.3065389  0.3348067  15.850   <2e-16 ***
    x           0.0036098  0.0252807   0.143    0.887    
    x2          0.0164635  0.0005694  28.913   <2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Residual standard error: 1.803 on 47 degrees of freedom
    Multiple R-squared:  0.9887,	Adjusted R-squared:  0.9882 
    F-statistic:  2053 on 2 and 47 DF,  p-value: < 2.2e-16
#+end_example
** Making Predictions
*** Making Predictions
**** 
     \small
    #+BEGIN_SRC R :session :exports code :results none graphics :file "./pdf_babel/quadratic_regression.pdf" :width 5 :height 3
    xv <- seq(-20,60,.5)
    yv <- predict(reg_quad,list(x=xv,x2=xv^2))
    ggplot(data=df[df$type=="quadratic",]) + theme_bw() +
      geom_hline(yintercept=0) + geom_vline(xintercept=0) +
      geom_point(aes(x=x,y=y),color="blue") +
      geom_line(data=data.frame(x=xv,y=yv),aes(x=x,y=y),color="red",size=1) 
    #+END_SRC
**** 
    #+BEGIN_CENTER
      #+ATTR_LATEX: :width .65\linewidth
      [[file:./pdf_babel/quadratic_regression.pdf]] 
    #+END_CENTER
** Confidence interval
*** Confidence interval
    Remember that
    #+BEGIN_EXPORT latex
    \begin{align*} 
     \hat\beta &  = \frac{ \sum_{i=1}^{n}{x_{i}y_{i}} - \frac1n \sum_{i=1}^{n}{x_{i}}\sum_{j=1}^{n}{y_{j}}}{ \sum_{i=1}^{n}({x_{i}^2}) - \frac1n (\sum_{i=1}^{n}{x_{i}})^2 } \\[6pt] 
     \hat\alpha  & = \bar{y} - \hat\beta\,\bar{x}
    \end{align*}
    #+END_EXPORT
    \vspace{-2em} 

    $\hat\beta$ and $\hat\alpha$ are sums of the $\epsilon_i$'s and it
    is thus possible to compute confidence intervals assuming:
    - the linear model holds true
    - either the errors in the regression are normally distributed
    - or the number of observations is sufficiently large so that the
      actual distribution of the estimators can be approximated using
      the central limit theorem
*** Illustration with R
**** The Anscombe quartet
    #+BEGIN_SRC R :session :exports both :results output
    head(a,10)
    #+END_SRC

#+LaTeX:
    #+RESULTS:
    #+begin_example
       idx set  x    y
    1    1   1 10 8.04
    2    1   2 10 9.14
    3    1   3 10 7.46
    4    1   4  8 6.58
    5    2   1  8 6.95
    6    2   2  8 8.14
    7    2   3  8 6.77
    8    2   4  8 5.76
    9    3   1 13 7.58
    10   3   2 13 8.74
#+end_example
#+LaTeX:\pause\vspace{-5.7cm}
**** Confidence intervals with ggplot
     :PROPERTIES:
     :BEAMER_act: <+>
     :END:
    #+BEGIN_SRC R :session :exports code :results output graphics :file "./pdf_babel/anscombe.pdf" :width 8 :height 3
    ggplot(data=a,aes(x=x,y=y)) + theme_bw() + 
      facet_wrap(~set,nrow=1) + geom_point(color="blue") + 
      geom_smooth(method='lm',color="red")
    #+END_SRC

    #+RESULTS:
    [[file:./pdf_babel/anscombe.pdf]]

    #+BEGIN_CENTER
       #+ATTR_LATEX: :width \linewidth
       [[file:./pdf_babel/anscombe.pdf]]
    #+END_CENTER
    \vspace{-1em}
* Extensions Linear model
** Discrete Variables: ANOVA
*** Confidence
If we had only 1 factor with 2 levels ($2^1$ design), the analysis
would simply amount to *compute confidence intervals* or more precisely
to *test whether $\boxed{\mu_{A=Low} = \mu_{A=High}}$ or not* (t-test)

\bgroup \scriptsize (if few observations are available we would have
to make the C.I wider and use the Student distribution) \egroup\bigskip

But when having more factors and/or levels, we want to test whether
*some of the combinations* have a significantly different expected value

| Number of comparisons       |  2 |     3 |     4 |     5 |     6 |
|-----------------------------+----+-------+-------+-------+-------|
| Nominal Type I error        | 5% |    5% |    5% |    5% |    5% |
| Actual overall Type I error | 5% | 12.2% | 20.3% | 28.6% | 36.6% |
\hfill (See 16.1.5 of [[https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf][/Practical Regression and Anova using R/]] by
Julian Faraway)\medskip
*** Quick illustration of the difficulty of multiple testing
\small
#+begin_src R :results output :session :exports both
se = .1; mean = 7 ;
N = 10000;
df = data.frame(
    x1=rnorm(N,mean=mean,sd=se),
    x2=rnorm(N,mean=mean,sd=se),
    x3=rnorm(N,mean=mean,sd=se))
df$eq1 = abs(df$x1-mean)<2*se
df$eq2 = abs(df$x1-df$x2)<2*se
df$eq3 = abs(df$x1-df$x2)<2*se & 
         abs(df$x1-df$x3)<2*se & 
         abs(df$x2-df$x3)<2*se

mean(df$eq1)
mean(df$eq2)
mean(df$eq3)
#+end_src

#+RESULTS:
: [1] 0.9538
: [1] 0.8435
: [1] 0.6596

*** Analysis of Variance (ANOVA)
#+BEGIN_SRC dot :file fig/var_anova.pdf :results output graphics :exports none
# From   http://www.unt.edu/rss/class/mike/5710/FactorialAnova.pdf
digraph G {
	node [color=black,
	      fillcolor=white,
	      shape=rectangle,
	      style=filled,
	      fontname="Helvetica"
	      ];

	      Tot_Var[label="Total Variability"];
	      Block_Var[label="Variability between blocks"];
	      BBlock_Var[label="Variability within blocks"];
	      A_Var[label="Variability of Factor A"];
	      B_Var[label="Variability of Factor B"];
	      AB_Var[label="Variability of Interaction"];
	      Tot_Var->Block_Var;
	      Tot_Var->BBlock_Var;
	      Block_Var->A_Var;
	      Block_Var->B_Var;
	      Block_Var->AB_Var;
}
#+END_SRC

#+RESULTS:
[[file:fig/var_anova.pdf]]


ANOVA (*ANalysis Of VAriance*) enable to *discriminate real effects from
noise* \vspace{-.4em}
- Enables to prove that *some parameters have little influence* and can
  be randomized over (possibly with a more elaborate model)
- Decomposes variance:\vspace{-1em}
  #+BEGIN_CENTER
  #+ATTR_LATEX: :width \linewidth
  file:fig/var_anova.pdf
  \vspace{-2em}
  #+END_CENTER
  - Assumes *identical standard deviation* for the populations
    (homoscedastic)
  - *Multiple tests at once* (assuming *normality*):
    $\boxed{\mu_{A=Low,*}-\mu_{A=High,*}=0}$,
    $\boxed{\mu_{B=Low,*}-\mu_{B=High,*}=0}$, \dots
*** ANOVA and F-statistic
The ANOVA produces an *F-statistic*, the ratio of the *variance
calculated among the means* to the *variance within the samples*.
- If the group means are drawn from populations with the same mean
  values, the *variance between the group means* should be *lower* than
  the *variance of the samples*
- A higher ratio therefore implies that the samples were drawn from
  populations with different mean values
\pause

Let's work out a simple made-up example
#+begin_src R :results output :session :exports both
Response = 10 + 2*as.numeric(d1$A) + 
    3*as.numeric(d1$B)*as.numeric(d1$C) + rnorm(nrow(d1))
d1 <- add.response(d1,Response, replace=TRUE)
#+end_src

I had to use =as.numeric= to interpret the $-1$ and $1$ as numbers
whereas they were created as *factors*

*** A simple ANOVA in R
\small
#+begin_src R :results output :session :exports both
d1_aov <- aov(Response ~ (A + B + C)^2, data=d1)
summary(d1_aov) # summary will call summary.aov
#+end_src

#+RESULTS:
#+begin_example
            Df Sum Sq Mean Sq F value   Pr(>F)    
A            1  22.98   22.98  38.318 0.000161 ***
B            1  68.02   68.02 113.417 2.11e-06 ***
C            1  77.60   77.60 129.402 1.21e-06 ***
A:B          1   0.44    0.44   0.728 0.415721    
A:C          1   0.93    0.93   1.555 0.243804    
B:C          1  14.62   14.62  24.374 0.000806 ***
Residuals    9   5.40    0.60                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

\medskip\normalsize
So, *all factors are significant* and there is *a significant
interaction between B and C*
*** Can't I just read my linear regression as usual?
\scriptsize

#+begin_src R :results output :session :exports both
summary.lm(d1_aov)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm.default(formula = Response ~ (A + B + C)^2, data = d1)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.01845 -0.48073 -0.01537  0.45886  0.98771 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  19.5912     0.1936 101.194 4.56e-15 ***
A1            1.1984     0.1936   6.190 0.000161 ***
B1            2.0618     0.1936  10.650 2.11e-06 ***
C1            2.2023     0.1936  11.375 1.21e-06 ***
A1:B1         0.1652     0.1936   0.853 0.415721    
A1:C1         0.2415     0.1936   1.247 0.243804    
B1:C1         0.9558     0.1936   4.937 0.000806 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7744 on 9 degrees of freedom
Multiple R-squared:  0.9716,	Adjusted R-squared:  0.9527 
F-statistic:  51.3 on 6 and 9 DF,  p-value: 1.873e-06
#+end_example

#+BEGIN_EXPORT latex
\pause
\begin{overlayarea}{.6\linewidth}{0cm}
  \vspace{-7.3cm}
  \begin{block}{}
    \normalsize
    \centerline{Wait, why is the formula so different?} $$10 + 2A + 3BC$$
  \end{block}
\end{overlayarea}
\pause
\begin{overlayarea}{.6\linewidth}{0cm}
  \vspace{-2.3cm}
  \begin{block}{}
    \normalsize
    \begin{center}
       Because it treated the factors "-1" and "1" as $0$ and $1$...
    \end{center}
  \end{block}
\end{overlayarea}

#+END_EXPORT
*** Then how do I get the formula I expected? (1/2)
\small
#+begin_src R :results output :session :exports both
d1_lm <- lm(Response ~ (as.numeric(A) + as.numeric(B) + 
            as.numeric(C))^2, data=d1)
summary.aov(d1_lm)
#+end_src

#+RESULTS:
#+begin_example
                            Df Sum Sq Mean Sq F value   Pr(>F)    
as.numeric(A)                1  22.98   22.98  38.318 0.000161 ***
as.numeric(B)                1  68.02   68.02 113.417 2.11e-06 ***
as.numeric(C)                1  77.60   77.60 129.402 1.21e-06 ***
as.numeric(A):as.numeric(B)  1   0.44    0.44   0.728 0.415721    
as.numeric(A):as.numeric(C)  1   0.93    0.93   1.555 0.243804    
as.numeric(B):as.numeric(C)  1  14.62   14.62  24.374 0.000806 ***
Residuals                    9   5.40    0.60                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#+end_example

\normalsize
Sweet, it's the same as the previous ANOVA
*** Then how do I get the formula I expected? (2/2)
\scriptsize
#+begin_src R :results output :session :exports both
summary(d1_lm) # summary will call summary.lm
#+end_src

#+RESULTS:
#+begin_example

Call:
lm.default(formula = Response ~ (as.numeric(A) + as.numeric(B) + 
    as.numeric(C))^2, data = d1)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.01845 -0.48073 -0.01537  0.45886  0.98771 

Coefficients:
                            Estimate Std. Error t value Pr(>|t|)    
(Intercept)                  15.4654     3.1870   4.853 0.000905 ***
as.numeric(A)                -0.0429     1.6878  -0.025 0.980277    
as.numeric(B)                -2.6022     1.6878  -1.542 0.157516    
as.numeric(C)                -2.7789     1.6878  -1.647 0.134064    
as.numeric(A):as.numeric(B)   0.6606     0.7744   0.853 0.415721    
as.numeric(A):as.numeric(C)   0.9658     0.7744   1.247 0.243804    
as.numeric(B):as.numeric(C)   3.8232     0.7744   4.937 0.000806 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7744 on 9 degrees of freedom
Multiple R-squared:  0.9716,	Adjusted R-squared:  0.9527 
F-statistic:  51.3 on 6 and 9 DF,  p-value: 1.873e-06
#+end_example

#+BEGIN_EXPORT latex
\pause
\begin{overlayarea}{.6\linewidth}{0cm}
  \vspace{-8.7cm}
  \begin{block}{}
    \begin{center}
      \normalsize Variability is too large too obtain good
      estimates of the \alert{true coefficients}\\[-2em]
      $$10 + 2A + 3BC$$

      \alert{One should anyway use other kind of designs to
         estimate continuous model parameters}
    \end{center}
  \end{block}
\end{overlayarea}
#+END_EXPORT
*** The difference between ANOVA and Linear Regression (3/3)
#+BEGIN_CENTER
\includegraphics[width=.9\linewidth]{fig/factor_impact.fig}
#+END_CENTER

- The coding numbers are completely meaningless and influence the
  estimates of the slope
  - If your input parameters are numerical, go for */extreme/
    values*, hoping the intermediate behavior is not too complicated
    and *consider them as factors*
- Real question: is a there *significant increase* when changing factors?
- Remember: you should use *ANOVA* for *factorial designs*, not LM
  - So don't use =summary.lm= in such cases; use =summary.aov=

*** And graphically ?
#+BEGIN_CENTER
#+begin_src R :results output graphics :file pdf_babel/doe1_ME.pdf :exports both :width 6 :height 4 :session
MEPlot(d1, abbrev=4, select=c(1,2,3), response="Response")
#+end_src

#+ATTR_LATEX: :width .8\linewidth
#+RESULTS:
[[file:pdf_babel/doe1_ME.pdf]]
#+END_CENTER

No CI on this one but we've seen that computing CIs is not
straightforward $\leadsto$ rely on the =summary.aov=
*** plotmeans                                                    :noexport:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session
plotmeans(data=d1, Response~A)
#+end_src

#+RESULTS:
[[file:/tmp/babel-25532x_l/figure25532hsX.png]]

*** What about interactions ?
#+BEGIN_CENTER
#+begin_src R :results output graphics :file pdf_babel/doe1_IA.pdf :exports both :width 6 :height 4 :session
IAPlot(d1, abbrev=4, show.alias=FALSE, select=c(1,2,3))
#+end_src

#+ATTR_LATEX: :width .8\linewidth
#+RESULTS:
[[file:pdf_babel/doe1_IA.pdf]]

#+END_CENTER

Again, no CI $\leadsto$ rely on the =summary.aov=

*** Checking hypothesis
\small

#+BEGIN_CENTER

#+begin_src R :results output graphics :file pdf_babel/doe1_check.pdf :exports both :width 8 :height 6 :session
layout(matrix(c(1,2,3,4),2,2)) # optional layout 
plot(aov(Response ~ (A + B + C)^2, data=d1))
#+end_src

#+ATTR_LATEX: :width .8\linewidth
#+RESULTS:
[[file:pdf_babel/doe1_check.pdf]]

#+END_CENTER

*** How do you expect me to ever remember all this ?
#+BEGIN_EXPORT latex
\begin{columns}
  \begin{column}{.55\linewidth}
    For the R commands, there is a trick: \winkey
    \begin{center}
      \bf \alert{Use Rcmdr and RcmdrPlugin.DoE}\\
      (by Ulrike Grömping)
    \end{center}
    Simply \texttt{library(RcmdrPlugin.DoE)}\dots
  \end{column}
  \begin{column}{.45\linewidth}
    \includegraphics[width=\linewidth]{images/rcmdr_doe.png}
  \end{column}
\end{columns}
#+END_EXPORT
You should only remember the principles and try to understand the
underlying hypothesis
- ANOVA enables to *discriminate real effects from noise* in *factorial
  experiments*. \bgroup\small /It relies on homoscedasticity and
  normality (or requires large number of samples)/\egroup
- *2-level factorial designs* are a simple way to go and are more
  efficient than OFAT experiments
- *Replicate thoroughly* and *randomize properly*: you will not go far
  wrong
*** Other example ?                                              :noexport:
Good and worked out example in
http://web.grinnell.edu/individuals/kuipers/stat2labs/Handouts/DOE%20Introductionh.pdf
** Generalized Linear Model
*** Linear model vs. Generalized linear Model
- Linear model :: $\displaystyle \rv{Y} = X.b+ \rv{\epsilon}$ with
  $\rv{\epsilon} \sim \mathcal{N}_n(0_n, \sigma^2 I)$

  $$\rv{\hat\beta}=(X^T.X)^{-1}.X^T.\rv{Y}$$

  The least squares regression is the Best Linear Unbiased Estimate
  ($\E[\rv{\hat\beta}] = b$ and $\Var(\rv{\hat\beta})$ is minimal.


  Unsuited when outcome $\rv{Y}$ is discrete or has unequal variance.\bigskip
- Generalized linear model :: \rv{Y} is assumed to be generated from a
     particular /distribution/ in the exponential family (\bgroup\small normal, binomial,
     Poisson, gamma...\egroup)\vspace{-1em}
  - $\displaystyle \E[\rv{Y}] = g^{-1}(X.b)$ ($g$ is the link
    function)
  - This allows variance to depend on the prediction ($\displaystyle
    \Var(\rv{Y}) = g^{-1}(X.b)$)
  
    | Distribution                        | Link function $g$                                |
    |-------------------------------------+--------------------------------------------------|
    | Binomial $(\{0,1\})$                | $\text{logit}(\mu)=\ln\left(\frac{\mu}{1-\mu}\right)$ |
    | Poisson $(\{0,1,2,\ldots\})$        | $\ln(\mu)$                                         |
    | Inverse Gaussian $(\mathbb{R}^*_+)$ | $\frac{1}{\mu^2}$                                  |

* Conclusion
**  
*** Conclusion
    1. You need a model to perform your regression
    2. You need to *check* whether the underlying *hypothesis* of this
       model are reasonable or not
    
    This model will allow you to:
    1. *Assess* and *quantify the effect* of parameters on the response
       - Parameters are estimated as a whole, using \textbf{all} the
         measurements
    2. *Extrapolate within the range* of parameters you tried
    3. Detect *outstanding* points (those with a high residual and/or
       with a high lever)

    This model will guide on how to design your experiments:
    - e.g., the linear model assumes some *uniformity* of interest
      over the parameter space range
    - if your system is heteroscedastic, you should perform more
      measurements for parameters that lead to higher variance
